---
title: "Homework I – ARIMA Model"
authors: "Behrouz Delfanian, Panagiotis Valsamis"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r load libraries, include=FALSE}
library(readxl)
library(ggplot2)
library(stats)
library(forecast)
library(ggpubr)
library(fpp3)
library(aTSA)
library(tseries)
library(urca)
```

The following code chunk loads data from a `.xlsx` file and creates a quarterly time series object `y` using the `date` and `opcnet` columns.\

It also checks if first element in `date` is already a `Date` or `POSIXt` object. If not (e.g., if it's a string or numeric from Excel), converts it to a `Date` object using the format `%d/%m/%Y` (day/month/year, e.g., "01/01/2007" becomes a `Date`).

```{r import data}
df <- read_excel("../data/data_homework_01.xlsx", sheet = "data_y") |>
  select(date, opcnet) |> 
  tidyr::drop_na()

# extract the starting date
first_date <- df$date[1]
if (!inherits(first_date, c("Date", "POSIXt"))) {
  first_date <- as.Date(first_date, format = "%d/%m/%Y")
}

year <- as.numeric(format(first_date, "%Y"))
month <- as.numeric(format(first_date, "%m"))
quarter <- ceiling(month / 3)
# month 1 → quarter 1; month 4 → quarter 2

ts_data <- ts(df$opcnet, start = c(year, quarter), frequency = 4)
head(ts_data, 12)
```
The `ts()` function in R creates a time series object that is a numeric vector (or matrix for multivariate data) with a "ts" class, equipped with attributes like `tsp` (specifying start, end, and frequency), ensuring equispaced observations for time-based analysis. It supports specialized methods for plotting, forecasting, and statistical tests while maintaining chronological integrity during operations like subsetting or arithmetic.

# Stationarity and Transformations

## Plot the series and its ACF & PACF
**ACF** (Autocorrelation Function) measures the correlation between a time series and its lagged versions, **helping identify patterns like seasonality or trends**.

**PACF** (Partial Autocorrelation Function) measures the correlation between the series and its lags **after removing the effects of intermediate lags**, useful for determining the order of autoregressive terms in models like ARIMA.

```{r plot_series_acf_pacf}
max_lag = 18

plot_ts <- ggplot(data.frame(x = time(ts_data), y = as.numeric(ts_data)), aes(x = x, y = y)) +
  geom_line(color = "black", size = 0.5) +
  labs(title = "opcnet Time Series", x = "Time (Years)", y = "opcnet ") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

plot_ac <- ggAcf(ts_data, lag.max = max_lag) + 
  labs(title = "ACF") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

plot_pac <- ggPacf(ts_data, lag.max = max_lag) + 
  labs(title = "PACF") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

ggarrange(plot_ts)
ggarrange(plot_ac)
ggarrange(plot_pac)

```

`Lag.max` specifies the maximum number of lags to display in ACF/PACF plots; for `N=75` observations, a common default (e.g., in R's `acf()` function) is `floor(10 * log10(75)) ≈ 18`, which balances showing enough lags to **identify patterns** like AR/MA orders or seasonality while avoiding unreliable high-lag correlations due to reduced effective sample size. Keep it below `N/4 (≈18.75)` to maintain statistical reliability.

**Interpretation**

- **Time Series Plot**:\
Stationary if mean and variance appear constant over time (no obvious trend, seasonality, or changing spread).\
Non-stationary if there's a trend (up/down drift), seasonality (repeating patterns), or heteroscedasticity (varying volatility).
- **ACF Plot**:\
Stationary if correlations **decay quickly to zero**.\
Non-stationary if they decay slowly or remain high over many lags (indicating persistence like unit root).
- **PACF Plot**:\
Stationary if partial correlations cut off after a few lags (suggesting finite AR order).\
Non-stationary if they show gradual decay or significance at many lags.

>Based on the explanations provided above, we may conclude that the `opcnet` time series appears to be *stationary*; however, further testing is required to confirm this.

## Test the series for stationarity

In time series analysis, assessing **stationarity** is crucial because many statistical models assume that the mean, variance, and autocorrelation structure of the series do not change over time. A common source of non-stationarity is the presence of a **unit root**, where shocks to the series have **permanent effects** and the series exhibits a stochastic trend rather than reverting to a fixed mean. The non-stationarity suggests **differencing** or **detrending** may be necessary to achieve stationarity before further modeling.

### ADF Test

The **Augmented Dickey–Fuller (ADF) test** is the most commonly used unit **root test**.

**Hypotheses**

- **Null hypothesis ($H_0$)**:\
  The series has a unit root → **non-stationary**  
- **Alternative hypothesis ($H_1$)**:\
  The series is stationary (or trend-stationary)

**Test Equation Variants**\
1. No constant, no trend  
2. With constant (most common)  
   $$\Delta y_t = \alpha + (\rho-1)y_{t-1} + \sum_{i=1}^p \gamma_i \Delta y_{t-i} + \varepsilon_t$$
3. With constant + linear trend

**Performing the ADF test using `aTSA` library:**
```{r stationarity, ADF test, aTSA}
aTSA::adf.test(ts_data, nlag = 10)
```

`aTSA::adf.test()` provides three types of tests. For *net issuance of assets or shares by investment funds, `opcnet`*, the most suitable ADF specification is usually: **Type 2: with drift, no trend**

Reasoning:\

- Economic/financial context:
  - Net issuance data is typically **mean-reverting around a non-zero level** (funds issue or redeem shares around some typical size).
  - It rarely exhibits a strong deterministic linear trend over time (which would justify Type 3).

- Type 1 (no drift, no trend):
  - Assumes the series fluctuates around zero.  
  - Not realistic for financial flows, which often have a positive average.

- Type 3 (drift + trend):
  - Assumes the series has a deterministic upward or downward trend.  
  - Only appropriate if net issuance is systematically increasing or decreasing over time.  
  - Most fund issuance series fluctuate around a stable mean with random shocks.


**Performing the ADF test using `tseries` library:**
```{r stationarity, ADF test, tseries}
tseries::adf.test(ts_data)
```

`tseries::adf.test()` runs a single ADF test.  

By default:

- Includes a drift (intercept)
- No deterministic trend
- Automatically chooses the number of lags (can be set with `k`)

So in effect, it is **similar to Type 2 (drift, no trend)** of `aTSA::adf.test()`.

**Test Interpretation**

| ADF p-value       | Decision                          |
|-------------------|-----------------------------------|
| p ≤ 0.05          | Reject $H_0$ → Stationary         |
| p > 0.05          | Fail to reject → Unit root likely |

>The ADF test was applied under different deterministic specifications and lag lengths. While the null of a unit root is rejected for small to moderate lag orders when allowing for a drift, the result is sensitive to lag selection and disappears for larger lag lengths. Overall, **the evidence for stationarity is weak and not robust**.

To obtain a robust assessment, we applied the ADF test using `urca::ur.df()` with `type = "drift"` and lag selection based on `AIC`. 

```{r stationarity, ADF test, urca}
summary(urca::ur.df(ts_data, type = "drift", selectlags = "AIC"))
```

Using the Augmented Dickey–Fuller test with a drift, the null hypothesis of a unit root is rejected at the 1% significance level. **The series is stationary around a deterministic mean**.

Putting everything together:

- The unit root null hypothesis is rejected
- The drift term is significant
- No trend is included or needed

#### Final ADF Test Conclusion
>To assess the stationarity of the series, we first applied the Augmented Dickey–Fuller (ADF) test using both `aTSA::adf.test()` and `tseries::adf.test()`. With `aTSA`, we examined three deterministic specifications (no drift, drift only, and drift with trend) across a range of lag lengths. The results indicated that the rejection of the unit root was sensitive to lag choice: the null hypothesis was rejected for small to moderate lags when allowing for a drift, but the effect disappeared for larger lag lengths, suggesting that the evidence for stationarity was weak and not robust. To obtain a more reliable assessment, we then used `urca::ur.df()` with `type = "drift"` and lag selection based on AIC. This method accounts for finite-sample critical values and separates the test for a unit root from the significance of the drift term, providing a defensible conclusion. Using this approach, the series was found to be stationary around a non-zero mean (drift-stationary).


### KPSS Test

The **Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test** tests for **stationarity** around a deterministic trend or level — the opposite hypothesis of ADF/PP tests.

**Hypotheses (reversed compared to ADF/PP!)**

- **Null hypothesis ($H_0$)**:\
  The series is **stationary** (level or trend-stationary)  
- **Alternative hypothesis ($H_1$)**:\
  The series has a **unit root** (non-stationary)

**Test Types**

- `"mu"` → tests stationarity around a **constant** (level-stationary)  
- `"tau"` → tests stationarity around a **linear trend** (trend-stationary)

**Decision Rule**

| KPSS p-value       | Conclusion                          |
|--------------------|-------------------------------------|
| p > 0.05           | Fail to reject $H_0$ → Stationary   |
| p ≤ 0.05           | Reject $H_0$ → Non-stationary       |

**Recommended Joint Testing Strategy**

| ADF/PP Result       | KPSS Result         | Conclusion                          |
|---------------------|---------------------|-------------------------------------|
| Reject $H_0$ (stationary) | Fail to reject $H_0$ (stationary) | Strong evidence of stationarity     |
| Fail to reject (unit root) | Reject $H_0$ (non-stationary)     | Strong evidence of unit root     |
| Conflicting results |                                     | Inconclusive → try differencing or other tests |


```{r stationarity, kpss test}
kpss.test(ts_data)
```

#### Final KPSS Test Conclusion

>The KPSS test fails to reject the null hypothesis of level stationarity (p-value ≥ 0.10). This indicates that there is no statistical evidence against the series being stationary around a constant. When combined with the ADF results, this provides consistent evidence that the series is stationary.

### PP Test
The **Phillips–Perron (PP) test** is a popular unit root test used to determine whether a time series is stationary or contains a unit root.

**Hypotheses**

- **Null hypothesis ($H_0$)**:\
  The series has a unit root → *non-stationary*  
- **Alternative hypothesis ($H_1$)**:\
  The series is (trend-)stationary → no unit root

**Key Advantages over ADF**

| Feature                        | ADF Test                          | PP Test                                      |
|-------------------------------|-----------------------------------|-----------------------------------------------|
| Correction for serial correlation | Parametric (adds lagged Δy)      | Non-parametric (Newey–West HAC estimator)    |
| Robustness to heteroskedasticity | No                                | Yes                                           |
| Small-sample performance       | Generally better                  | Slightly lower power, but more robust         |

**Test Specifications (choose one)**

1. **No constant, no trend**  
2. **With constant (drift)** → most common for economic series  
3. **With constant + linear trend** → use when series shows clear trend

**Decision Rule**

| PP p-value       | Conclusion                          |
|--------------------|-------------------------------------|
| p-value < 0.05           | Reject $H_0$ → series is stationary   |
| p-value > 0.05           | Fail to reject $H_0$ → evidence of unit root (non-stationary)       |

```{r stationarity, pp test}
# Similar to Augmented DF test, performs worse than ADF in small samples
pp.test(ts_data)
```

#### Final PP Test Conclusion
>The Phillips–Perron test rejects the null hypothesis of a unit root at the 1% significance level (p ≤ 0.01). This provides evidence that the series is stationary. Since the PP test is robust to serial correlation and heteroskedasticity, this result supports and strengthens the conclusions obtained from the ADF and KPSS tests.

### Comparing All Tests

```{r stationarity, compare all}
stationary.test(ts_data, method = c("adf", "pp", "kpss"))
```

#### Final Stationarity Assessment

The stationarity of the series was assessed using multiple complementary tests, including the Augmented Dickey–Fuller (ADF), Phillips–Perron (PP), and KPSS tests.\

- The ADF test results, across different deterministic specifications and moderate lag lengths, generally reject the null hypothesis of a unit root when a drift is included.\
- The Phillips–Perron test also rejects the null hypothesis of a unit root at the 1% significance level, providing robust evidence of stationarity while accounting for serial correlation and heteroskedasticity.\
- In contrast, the KPSS test fails to reject the null hypothesis of level stationarity, indicating no evidence against stationarity around a constant.

Taken together, these results provide consistent evidence that the series is **stationary around a non-zero mean (drift-stationary)**. Although the ADF test shows some sensitivity to lag length in exploratory settings, the results obtained using `urca::ur.df()` with lag selection based on AIC, combined with the PP and KPSS tests, support the conclusion that the series does not contain a unit root.

#### Implications for ARIMA Modelling

Since the series is already stationary in levels, **no differencing is required** to achieve stationarity. Applying first differences or growth rates would therefore **risk overdifferencing** and removing meaningful dynamics from the data. Consequently, the original series will be used directly for ARIMA modelling. As a robustness check, stationarity tests may be re-applied after any transformation considered during model diagnostics.

```{r transformations}
diff_data <- diff(ts_data)
aTSA::adf.test(diff_data, nlag = 10)
```

**Interpretation:**\

>The ADF test applied to the first-differenced series strongly rejects the null hypothesis of a unit root across all deterministic specifications and lag lengths, with p-values uniformly below 1%. This indicates that **the differenced series is clearly stationary**. However, since previous ADF, PP, and KPSS tests already provided evidence that the original series is stationary in levels, **differencing is not required to achieve stationarity**. Consequently, applying first differences would likely result in overdifferencing. For ARIMA modelling, the series will therefore be used in levels with integration order \( d = 0 \).

# ARIMA Model

Using the stationary series obtained in Section 1, estimate ARMA models using the first 80% of the sample.  
Select an ARIMA model that you will use for forecasting.

```{r arima_modeling}
# Code to split data, estimate ARIMA models, and select one
# Example:
# n <- length(ts_data)
# train_size <- floor(0.8 * n)
# train_data <- ts_data[1:train_size]
# model <- auto.arima(train_data)
# summary(model)
```

## Model Choice

[Include all relevant elements that support your model choice, such as:  
- regression output  
- information criteria tables  
- residual diagnostics and residual plots  
- ACF/PACF of residuals  
- test results for autocorrelation, normality, heteroskedasticity, etc.  
Via the code chunk output.]

Justify your selected model by referencing the evidence from the tables and figures.

# Forecast

Using your chosen model from Section 2, perform one-step-ahead recursive forecasts (expanding-window forecasts) for the remaining 20% of the sample.

```{r forecasting}
# Code for one-step-ahead recursive forecasts
# Example: Refer to “Ex_uni_TS_realData.Rmd”
# forecasts <- numeric(test_size)
# for(i in 1:test_size) {
#   train <- ts_data[1:(train_size + i - 1)]
#   model <- Arima(train, order = c(p, d, q))
#   forecasts[i] <- forecast(model, h=1)$mean
# }
# plot(ts_data[(train_size+1):n], type="l")
# lines(forecasts, col="red")
```

[Plot the forecasts together with the true observed values via the code chunk output.]

Provide a brief commentary:  
- Does the model seem to forecast well or poorly?  
- Are there periods where performance improves or worsens?  
- Do you have an intuition for how the forecasts could be improved?  
Also report the RMSE of your one-step-ahead forecasts.

```{r rmse}
# Code to calculate RMSE
# rmse <- sqrt(mean((forecasts - ts_data[(train_size+1):n])^2))
# rmse
```

# Further Analysis

[Optional: Answer one or both of the following:]

- What are the consequences of estimating an ARIMA model on non-stationary data?

- Clean the data for outliers or structural breaks (do not forget to include the code in the code file).

- Briefly explain why the additional series is economically linked to your target variable and how its timeliness (shorter publication lag) can provide useful, up-to-date information to improve your forecast.

```{r bonus_cleaning, eval=FALSE}
# Optional code for cleaning data
```