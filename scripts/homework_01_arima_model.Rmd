---
title: "Homework I – ARIMA Model"
authors: "Behrouz Delfanian, Panagiotis Valsamis"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r load libraries, include=FALSE}
library(readxl)
library(ggplot2)
library(stats)
library(forecast)
library(ggpubr)
library(fpp3)
library(aTSA)
library(tseries)
library(urca)
```

The following code chunk loads data from a `.xlsx` file and creates a quarterly time series object `y` using the `date` and `opcnet` columns.\

It also checks if first element in `date` is already a `Date` or `POSIXt` object. If not (e.g., if it's a string or numeric from Excel), converts it to a `Date` object using the format `%d/%m/%Y` (day/month/year, e.g., "01/01/2007" becomes a `Date`).

```{r import data}
df <- read_excel("../data/data_homework_01.xlsx", sheet = "data_y") |>
  select(date, opcnet) |> 
  tidyr::drop_na()

# extract the starting date
first_date <- df$date[1]
if (!inherits(first_date, c("Date", "POSIXt"))) {
  first_date <- as.Date(first_date, format = "%d/%m/%Y")
}

year <- as.numeric(format(first_date, "%Y"))
month <- as.numeric(format(first_date, "%m"))
quarter <- ceiling(month / 3)
# month 1 → quarter 1; month 4 → quarter 2

ts_data <- ts(df$opcnet, start = c(year, quarter), frequency = 4)
head(ts_data, 12)
```
The `ts()` function in R creates a time series object that is a numeric vector (or matrix for multivariate data) with a "ts" class, equipped with attributes like `tsp` (specifying start, end, and frequency), ensuring equispaced observations for time-based analysis. It supports specialized methods for plotting, forecasting, and statistical tests while maintaining chronological integrity during operations like subsetting or arithmetic.

## Stationarity and Transformations

### Plot the series and its ACF & PACF
**ACF** (Autocorrelation Function) measures the correlation between a time series and its lagged versions, **helping identify patterns like seasonality or trends**.

**PACF** (Partial Autocorrelation Function) measures the correlation between the series and its lags **after removing the effects of intermediate lags**, useful for determining the order of autoregressive terms in models like ARIMA.

```{r plot_series_acf_pacf}
max_lag = 18

plot_ts <- ggplot(data.frame(x = time(ts_data), y = as.numeric(ts_data)), aes(x = x, y = y)) +
  geom_line(color = "black", size = 0.5) +
  labs(title = "opcnet Time Series", x = "Time (Years)", y = "opcnet ") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

plot_ac <- ggAcf(ts_data, lag.max = max_lag) + 
  labs(title = "ACF") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

plot_pac <- ggPacf(ts_data, lag.max = max_lag) + 
  labs(title = "PACF") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

ggarrange(plot_ts)
ggarrange(plot_ac)
ggarrange(plot_pac)

```

`Lag.max` specifies the maximum number of lags to display in ACF/PACF plots; for `N=75` observations, a common default (e.g., in R's `acf()` function) is `floor(10 * log10(75)) ≈ 18`, which balances showing enough lags to **identify patterns** like AR/MA orders or seasonality while avoiding unreliable high-lag correlations due to reduced effective sample size. Keep it below `N/4 (≈18.75)` to maintain statistical reliability.

**Interpretation**

- **Time Series Plot**:\
Stationary if mean and variance appear constant over time (no obvious trend, seasonality, or changing spread).\   Non-stationary if there's a trend (up/down drift), seasonality (repeating patterns), or heteroscedasticity (varying volatility).
- **ACF Plot**:\
Stationary if correlations **decay quickly to zero**.\
Non-stationary if they decay slowly or remain high over many lags (indicating persistence like unit root).
- **PACF Plot**:\
Stationary if partial correlations cut off after a few lags (suggesting finite AR order).\
Non-stationary if they show gradual decay or significance at many lags.

>Based of the explanations provided above we may conclude that the `opcnet`time series is stationary.

### Test the series for stationarity

#### ADF Test

The **Augmented Dickey–Fuller (ADF) test** is the most commonly used unit **root test**.

**Hypotheses**

- **Null hypothesis ($H_0$)**:\
  The series has a unit root → **non-stationary**  
- **Alternative hypothesis ($H_1$)**:\
  The series is stationary (or trend-stationary)

**Test Equation Variants**
1. No constant, no trend  
2. With constant (most common)  
   $$\Delta y_t = \alpha + (\rho-1)y_{t-1} + \sum_{i=1}^p \gamma_i \Delta y_{t-i} + \varepsilon_t$$
3. With constant + linear trend

```{r stationarity, ADF test}
aTSA::adf.test(ts_data, nlag = 10)
tseries::adf.test(ts_data)
#print("We cannot reject H0, which says that the series is non-stationary, regardless of the type of the model.")
```

**Test Interpretation**

| ADF p-value       | Decision                          |
|-------------------|-----------------------------------|
| p ≤ 0.05          | Reject $H_0$ → Stationary         |
| p > 0.05          | Fail to reject → Unit root likely |

The ADF test was applied under different deterministic specifications and lag lengths. While the null of a unit root is rejected for small to moderate lag orders when allowing for a drift, the result is sensitive to lag selection and disappears for larger lag lengths. Overall, the evidence for stationarity is weak and not robust.

```{r stationarity, ADF test, urca}
summary(urca::ur.df(ts_data, type = "drift", selectlags = "AIC"))
```

Using the Augmented Dickey–Fuller test with a drift, the null hypothesis of a unit root is rejected at the 1% significance level. The series is stationary around a deterministic mean.

Putting everything together:

- The unit root null hypothesis is rejected
- The drift term is significant
- No trend is included or needed

#### KPSS Test

The **Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test** tests for **stationarity** around a deterministic trend or level — the opposite hypothesis of ADF/PP tests.

**Hypotheses (reversed compared to ADF/PP!)**

- **Null hypothesis ($H_0$)**:\
  The series is **stationary** (level or trend-stationary)  
- **Alternative hypothesis ($H_1$)**:\
  The series has a **unit root** (non-stationary)

**Test Types**

- `"mu"` → tests stationarity around a **constant** (level-stationary)  
- `"tau"` → tests stationarity around a **linear trend** (trend-stationary)

**Decision Rule**

| KPSS p-value       | Conclusion                          |
|--------------------|-------------------------------------|
| p > 0.05           | Fail to reject $H_0$ → Stationary   |
| p ≤ 0.05           | Reject $H_0$ → Non-stationary       |

**Recommended Joint Testing Strategy**

| ADF/PP Result       | KPSS Result         | Conclusion                          |
|---------------------|---------------------|-------------------------------------|
| Reject $H_0$ (stationary) | Fail to reject $H_0$ (stationary) | Strong evidence of stationarity     |
| Fail to reject (unit root) | Reject $H_0$ (non-stationary)     | Strong evidence of unit root     |
| Conflicting results |                                     | Inconclusive → try differencing or other tests |


```{r stationarity, kpss test}
kpss.test(ts_data)
print("We reject H0, which says that the series is stationary, regardless of the type of the model.")
```

#### PP Test
The **Phillips–Perron (PP) test** is a popular unit root test used to determine whether a time series is stationary or contains a unit root.

**Hypotheses**

- **Null hypothesis ($H_0$)**:\
  The series has a unit root → *non-stationary*  
- **Alternative hypothesis ($H_1$)**:\
  The series is (trend-)stationary → no unit root

**Key Advantages over ADF**

| Feature                        | ADF Test                          | PP Test                                      |
|-------------------------------|-----------------------------------|-----------------------------------------------|
| Correction for serial correlation | Parametric (adds lagged Δy)      | Non-parametric (Newey–West HAC estimator)    |
| Robustness to heteroskedasticity | No                                | Yes                                           |
| Small-sample performance       | Generally better                  | Slightly lower power, but more robust         |

**Test Specifications (choose one)**

1. **No constant, no trend**  
2. **With constant (drift)** → most common for economic series  
3. **With constant + linear trend** → use when series shows clear trend

**Decision Rule**

| KPSS p-value       | Conclusion                          |
|--------------------|-------------------------------------|
| p-value < 0.05           | Reject $H_0$ → series is stationary   |
| p-value > 0.05           | Fail to reject $H_0$ → evidence of unit root (non-stationary)       |

**Practical Recommendation**

Run **both ADF and PP** tests. Conclusions are strongest when both tests agree.

```{r stationarity, pp test}
# Similar to Augmented DF test, performs worse than ADF in small samples
# H0: UR
pp.test(ts_data)
print("We cannot reject H0, which says that the series is non-stationary, regardless of the type of the model.")
```

#### Comparing All Tests

```{r stationarity, compare all}
stationary.test(ts_data, method = c("adf", "pp", "kpss"))
```


[Insert the tables (ADF, KPSS, PP, …) directly after the subsection title via the code chunk output.]

Interpret the test results and conclude whether the series is stationary.  
Explain which transformation(s) (differencing, growth rates, seasonal differencing, etc.) you will apply to make the series stationary and ready for ARIMA modelling (do not forget to re-test the series after you transformed it to stationarity!).

```{r transformations}
# Code to apply transformations and re-test
# Example:
# diff_data <- diff(ts_data)
# adf.test(diff_data)
```

## ARIMA Model

Using the stationary series obtained in Section 1, estimate ARMA models using the first 80% of the sample.  
Select an ARIMA model that you will use for forecasting.

```{r arima_modeling}
# Code to split data, estimate ARIMA models, and select one
# Example:
# n <- length(ts_data)
# train_size <- floor(0.8 * n)
# train_data <- ts_data[1:train_size]
# model <- auto.arima(train_data)
# summary(model)
```

### Model Choice

[Include all relevant elements that support your model choice, such as:  
- regression output  
- information criteria tables  
- residual diagnostics and residual plots  
- ACF/PACF of residuals  
- test results for autocorrelation, normality, heteroskedasticity, etc.  
Via the code chunk output.]

Justify your selected model by referencing the evidence from the tables and figures.

## Forecast

Using your chosen model from Section 2, perform one-step-ahead recursive forecasts (expanding-window forecasts) for the remaining 20% of the sample.

```{r forecasting}
# Code for one-step-ahead recursive forecasts
# Example: Refer to “Ex_uni_TS_realData.Rmd”
# forecasts <- numeric(test_size)
# for(i in 1:test_size) {
#   train <- ts_data[1:(train_size + i - 1)]
#   model <- Arima(train, order = c(p, d, q))
#   forecasts[i] <- forecast(model, h=1)$mean
# }
# plot(ts_data[(train_size+1):n], type="l")
# lines(forecasts, col="red")
```

[Plot the forecasts together with the true observed values via the code chunk output.]

Provide a brief commentary:  
- Does the model seem to forecast well or poorly?  
- Are there periods where performance improves or worsens?  
- Do you have an intuition for how the forecasts could be improved?  
Also report the RMSE of your one-step-ahead forecasts.

```{r rmse}
# Code to calculate RMSE
# rmse <- sqrt(mean((forecasts - ts_data[(train_size+1):n])^2))
# rmse
```

## Further Analysis

[Optional: Answer one or both of the following:]

- What are the consequences of estimating an ARIMA model on non-stationary data?

- Clean the data for outliers or structural breaks (do not forget to include the code in the code file).

- Briefly explain why the additional series is economically linked to your target variable and how its timeliness (shorter publication lag) can provide useful, up-to-date information to improve your forecast.

```{r bonus_cleaning, eval=FALSE}
# Optional code for cleaning data
```