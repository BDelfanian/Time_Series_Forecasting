---
title: "Multivariate Series - Simulated Data"
output: html_notebook
author: Vasja Sivec
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Simulate and estimate multivariate time series models

The goal is to get acquainted with multivariate TS models and tests in a controlled environment (=simulated data). Since we control the environment (=the true data-generating process is known) we abstract from practical failures (e.g. outliers, breaks, model-uninformative data). Instead, we inspect properties of TS models and gain intuition.        

Exercise 1 

* Simulate data from a weakly stationary VAR(1) with vec(A) = [0.8,0,0.5,0.8]', $\Sigma=I$, without built-in simulation routines. Plot data.  

* Decide if the data is stationary or not.

* Inspect correlation coefficients

* Estimate a VAR model on simulated data. Estimate appropriate lag length. 

* Use eq. by eq. OLS and built-in routines to estimate a VAR(1). Discuss the differences. 

* Perform tests for: Granger causality, stability, whitness of the residuals,...  

From here on, you are required to complete parts of the code yourself (see items marked with [XXXXXXXXXXXXXX]).

```{r}
library(stats)
library(MASS)
# Set burn-in observations (to wash out the effect of initial values), the number of observations and number of series
set.seed(123)
nburn <- 100
nobs  <- 100
n     <- 2

# Set AR coefficient value
A <- matrix(c(0.8, 0, 0.5, 0.8), nrow = n, ncol = n, byrow = TRUE)

# Draw white-noise errors (N~(0,1))
Mu    <- c(0,0)
Sigma <- diag(2)
E     <- mvrnorm(nburn+nobs,Mu,Sigma) 
# cor(E)

# Empty place-holder for the AR series
y     <- matrix(NA, nrow = (nburn + nobs), ncol = n)

# Initialize the data matrix with 0s
y[1,] <- c(0,0)

# Simulate AR(1) process
for (i in 2:(nburn + nobs)) {
  y[i,] <- y[i-1,] %*% t(A) + E[i,]
}

# Drop burn-in observations
y <- y[(nburn + 1):(nburn + nobs),]

# Plot series
matplot(y, type = "l", main = "y1-blue, y2-orange")
legend("topright", legend = c("y1", "y2"), col = 1:2, lty = 1)

# Let us have a look at a few statistics
cor(y)
dim(y)
print("Can you determine from the figure whether the data is stationary and exhibits co-movement?")
```

```{r}
# Determine if the series are stationary or not
# 1. Plot AC and PAC
# Ex 2 stationarity
# Load required packages
library(ggplot2)
library(stats)
library(forecast)
library(ggpubr)

# Transform to data frame
df           <- as.data.frame(y)
colnames(df) <- c("y1","y2")
date         <- 1:nrow(df)
df           <- cbind(date,df)

# Create a function that plot ACF and PACF for a single series
plot_ac_pac <- function(df, series_name) {
plot_y  <- ggplot(data.frame(x=df$date, y=as.numeric(df[[series_name]])), aes(x = x, y = y))+ 
  geom_line() + 
  labs(title = toupper(series_name))
  plot_ac  <- ggAcf(df[[series_name]],lag.max = 10) + labs(title="ACF")
  plot_pac <- ggPacf(df[[series_name]],lag.max = 10) + labs(title="PACF")
  # Display plot
  ggarrange(plot_y,plot_ac,plot_pac,nrow = 3,ncol = 1)
}

# Plot AC and PAC functions - for individual stock index 
plot_ac_pac(df,"y1")
plot_ac_pac(df,"y2")

print("Do the data appear stationary? What do the ACF and PACF suggest? Is it evident whether they are stationary or not? If not, conduct a stationary test on the data.")
```

```{r}
# Determine if the series are stationary or not
# 2. Test for stationary
library(tseries)
library(aTSA)

# Create a data frame for storing p-values
ur_probs <- data.frame(adf = rep(0, 2), kpss = rep(0, 2))
row.names(ur_probs) <- c('y1', 'y2')

# Perform stationarity tests and store p-values in the dataframe
for (i in 2:ncol(df)) {  # Start from column 2 (excluding the date column)

  # ADF TEST (H0:?)
  adf_result <- aTSA::adf.test(na.omit(df[,i], nlag = NULL), output = FALSE)   
  # Which unit root type will you store here? Type1,..,Type3? Why
  ur_probs[i - 1, "adf"] <-tail(adf_result$type1[,3],1)
  # In principle, you should select the lag length based on the model's statistical adequacy,   considering white noise errors, the significance of coefficients, etc. In practice, we often   rely on the default lag length, which is calculated as floor(4*(length(x)/100)^(2/9)), and its results are stored as the last element.
  
  # KPSS TEST (H0:?)
  kpss_result <- aTSA::kpss.test(na.omit(df[,i], nlag = NULL), output = FALSE)
  ur_probs[i - 1, "kpss"] <- kpss_result[1,3]

}

# Print the table with p-values for ADF and KPSS tests
print("Table with p-values for ADF (H0: series is UR) and KPSS (H0: series is stat)")
print(ur_probs)
print("Which series in our dataset are stationary, and which are not?")
```

```{r}
# Perform the same tests but, first only on the first 50 observations and then on the observations after the first 50 observations. 

# Create a data frame for storing p-values
ur_probs <- data.frame(adf = rep(0, 2), kpss = rep(0, 2))
row.names(ur_probs) <- c('y1', 'y2')

# Perform stationarity tests and store p-values in the dataframe
for (i in 2:ncol(df)) {  # Start from column 2 (excluding the date column)
  # ADF TEST (H0:?)
  # adf_result <- aTSA::adf.test(na.omit(df[1:50, i]), nlag = NULL, output = FALSE)   
  adf_result <- aTSA::adf.test(na.omit(df[51:100, i]), nlag = NULL, output = FALSE)   
  ur_probs[i - 1, "adf"] <-tail(adf_result$type1[,3],1)
  # KPSS TEST (H0:?)
  # kpss_result <- aTSA::kpss.test(na.omit(df[1:50, i]), lag.short = TRUE, output = FALSE)
  kpss_result <- aTSA::kpss.test(na.omit(df[51:100, i]), lag.short = TRUE, output = FALSE)
  ur_probs[i - 1, "kpss"] <- kpss_result[1,3]
}
print(ur_probs)
print("What do the tests indicate? Should you model this smaller series as stationary or non-stationary? These tests may have limited power with small samples or highly persistent series. In such cases, it's advisable to rely on economic theory or basic logic rather than solely depending on the tests.")
```

```{r}
# Estimate correlation coefficients between y1 and y2
print(cor(na.omit(df[,-1])))
print("Contemporaneous correlation between y1 and y2 is very high.")
```

```{r}
# Estimate partial correlation coefficients between y1 and y2, conditioning on lag of y1 
library(ppcor)
library(dplyr)
df$Ly1 <- dplyr::lag(df$y1)
pcor(na.omit(cbind(df$y1,df$y2,df$Ly1)))
print("Partial correlation between y1 and y2, when controling for the lag of y1 is small. Could you deduce this from the model?")
```

```{r}
# Estimate partial correlation coefficients between y1 and lag of y2, conditioning on lag of y1. What result can you expect? 
df$Ly2 <- dplyr::lag(df$y2)
pcor(na.omit(cbind(df$y1,df$Ly2,df$Ly1)))
print("Could you have inferred this from the model? What would happen if we estimated the partial correlation coefficient between y2 and the lag of y1 while controlling for the lag of y2? The point is that (partial) correlation coefficients tell you a lot about how the models should look like.")
```

```{r}
# A better way to determine if one variable is a good predictor of another is by using Granger causality tests (This test has nothing to do with causality, its name is misleading!).
library(lmtest)
grangertest(y1 ~ y2, order = 1, data = df) # H0: y2 does not Granger cause y1
grangertest(y2 ~ y1, order = 1, data = df) # H0: y1 does not Granger cause y2
print("What do you conclude?")
```

```{r}
# Now select the appropriate lag length
library(vars)
ics <- VARselect(df[,2:3],lag.max = 5)
print(ics$selection)   
print("Are the results correct?")
print("Decrease the sample size to 40 and re-do the test..")
```

```{r}
# Since the purpose here is forecasting, let us evaluate the models with different lags based on pseudo-out-of-sample forecasts.
# Let us use 80% of the sample to estimate the model parameters and 1-step ahead forecasts.
# Save 20% of the sample to test the forecasts
df_var  <- subset(df, select = c("date","y1","y2"))
n_fcs   <- round(0.2 * nrow(df_var))
fcs     <- matrix(nrow = n_fcs, ncol = 2)

# Declare evaluation measures matrices
rmse <- matrix(nrow = 3, ncol = 2)
mae <- matrix(nrow = 3, ncol = 2)
rmspe <- matrix(nrow = 3, ncol = 2)

for (p in 1:3) {  # loop over models: p = [1, 2]

    
  for (i in 1:n_fcs) {  # loop for 1-step-ahead forecasts on the last 10% of the data
    
    # Estimate the model
    estMdl <- VAR(df_var[1:(nrow(df_var) - (n_fcs-i+1)),-1], p = p, type="const")  # fit VAR(p)
    
    # Predict
    fcs_i  <- predict(estMdl,n.ahead=1)
    
    # Store prediction
    fcs[i, 1] <- fcs_i$fcst$y1[1]  # store the forecast
    fcs[i, 2] <- fcs_i$fcst$y2[1]   # store the forecast
    }

  # Calculate rmse, rmspe, mae
  actual <- df_var[(nrow(df)-n_fcs+1):nrow(df),-1]
  errors <- actual - fcs
  rmse[p, 1:2] <- sqrt(colMeans(errors^2))
  mae[p, 1:2] <- colMeans(abs(errors))
  rmspe[p, 1:2] <- sqrt(colMeans((errors)/actual)^2)

  # plot(actual[,1],type="l")
  # lines(1:200,fcs[,1],col="red")
  
}


cat("RMSE\n")
colnames(rmse) <- c('y1', 'y2')
rownames(rmse) <- c('p=1', 'p=2', 'p=3')
print(rmse)
cat("MAE\n")
colnames(mae) <- c('y1', 'y2')
rownames(mae) <- c('p=1', 'p=2', 'p=3')
print(mae)
cat("RMSPE\n")
colnames(rmspe) <- c('y1', 'y2')
rownames(rmspe) <- c('p=1', 'p=2', 'p=3')
print(rmspe)
print("Which model would you choose according to this pseudo-forecasting comparison?")
# Reduce df_var to 100 time periods and re-do the exercise. How does this change your lag selection? 
```

```{r}
# Ex 5: Perform expanding-window one-step-ahead forecast 
for (i in 1:n_fcs) {  # loop for 1-step-ahead forecasts on the last 10% of the data
    
  # Predict
  estMdl <- VAR(df_var[[XXXXXXXXXXXXXX]], p = 1, type="const")  # fit VAR(1).   Model is re-estimated for each forecast period
  fcs_i  <- predict(estMdl,df_var[1:(nrow(df_var) - (n_fcs-i)-1),], ci = 0.9,n.ahead=1)
  if (i==1) {
  fcs <- fcs_i  # store the 1-step-ahead forecast for period i
  } else {
  fcs$fcst$y1 <- rbind(fcs$fcst$y1,fcs_i$fcst[[1]]) # attach the new 1-step ahead forecast 
  fcs$fcst$y2  <- rbind(fcs$fcst$y2,fcs_i$fcst[[2]]) # attach the new 1-step ahead forecast 
  }
}

# PLot
par(mar = c(1, 1, 1, 1))   # for some reason we have to change plot margins.. 
plot(fcs)

# Plot together with actual
true           <- df_var[,1:2]
fcs_plt        <- df_var[,1:2]
fcs_plt        <- cbind(fcs_plt,fcs=NA,lower=NA,upper=NA)
fcs_plt[(nrow(fcs_plt) - n_fcs +1):nrow(fcs_plt),3] <- fcs$fcst$y1[,1]
fcs_plt[(nrow(fcs_plt) - n_fcs +1):nrow(fcs_plt),4] <- fcs$fcst$y1[,2]
fcs_plt[(nrow(fcs_plt) - n_fcs +1):nrow(fcs_plt),5] <- fcs$fcst$y1[,3]

plot(fcs_plt$date[50:nrow(fcs_plt)], fcs_plt$y1[50:nrow(fcs_plt)], col="blue", type="l", lty=1)
lines(fcs_plt$date[50:nrow(fcs_plt)], fcs_plt$upper[50:nrow(fcs_plt)], col="dark red", lty=2)
lines(fcs_plt$date[50:nrow(fcs_plt)], fcs_plt$lower[50:nrow(fcs_plt)], col="dark red", lty=2)
lines(fcs_plt$date[150:nrow(fcs_plt)], fcs_plt$lower[50:nrow(fcs_plt)], col="dark red", lty=2)
points(fcs_plt$date[50:nrow(fcs_plt)], fcs_plt$fcs[50:nrow(fcs_plt)], col="red", pch="*")
lines(fcs_plt$date[50:nrow(fcs_plt)], fcs_plt$fcs[50:nrow(fcs_plt)], col="red",lty=2)
print("Are the forecasts precise? Not too bad. However, predicting stock markets will be a lot more difficult.")
```

```{r}
# In R you can also easily produce a nice "fan-chart" for the multi-step ahead forecast
fcs_multi  <- predict(estMdl,df_var[1:(nrow(df_var) - (n_fcs-i)-1),],n.ahead=10)
fcs_multi$endog <- tail(fcs_multi$endog,30)
fanchart(fcs_multi) 
```

```{r}
# Let us also have a closer look at the last estimated model
summary(estMdl)
# We see that none of the coefficients in the CAC equation are significant. If we care about CAC forecasts we could estimate increase the lag to 2. 
estMdl <- VAR(df_var[,2:3], p = 2, type="const")

# A bit nicer way of displaying the results
library(stargazer)
stargazer(estMdl$varresult,type="text",style="aer",title="VAR(2)",column.labels =c("luxx","cac"))
```

```{r}
# Let us check stability of the VAR model
# 1. We check if the coefficients in the var are stable over time
stability <- stability(estMdl)
par(mfrow=c(1,2))
plot(stability$stability$y1)
plot(stability$stability$y2)
# If the plotted cumsum statistic goes outside of the bounds, we reject the null hypothesis of parameter stability. 
# p-value for the stability test can be retrieved with the sctest function
sctest(estMdl)
```

```{r}
# Let's check if the residuals are well behaved
serial.test(estMdl, lags.pt=4)   # no AC in the residuals up to and including order lags.pt
```

```{r}
# Let us check if this VAR is dynamically stable/stationary.
# Transform VAR(2) into VAR(1) using the companion form and calculate the absolute eigenvalues. If any of them is greater than 1, then the model is nonstationary (unstable).
coeffs <- Acoef(estMdl)
A <- rbind(cbind(coeffs[[1]],coeffs[[2]]),cbind(diag(2),matrix(0, 2, 2)))
eigv <- eigen(A,only.values = TRUE)
abs(eigv$values)
# Are any of the eigenvalues larger than 1? If not, the model is stable. Why? If we have A = Qeigvinv(Q), for example, the forecast h periods ahead is y_T+h = A^h y_T. Now, let's replace A with eigen decomposition.
```

```{r}
# Plot the IRs
# Create the IR matrices.
#This one uses Cholesky decomposition (=ortho) of the variance-covariance matrix of the residuals:
irf <- irf(estMdl,n.ahead=6,ortho=TRUE,impulse=c("y1","y2"),response=c("y1","y2"))
plot(irf)
#CAUTION: R's  IRF function displays IRs differently than other packages. For example, 'Orthogonal IR from y1' means 'Orthogonal IR of y1 & y2 from the shocking error of the y1 equation.

```

```{r}
# Forecast error variance decomposition
fevd <- fevd(estMdl,n.ahead = 20)
par(mfrow=c(1,2),mar=c(2,2,2,2))
plot(fevd)
```

```{r}
# The above orthogonalized IRs impose an identifying restriction, suggesting that the 'y1' does not respond to shocks or unexpected events in the 'y2' on impact, while the 'y2' can immediately responds to unexpected events or shocks in 'y1'. Is this a reasonable assumption in this case?

# Rearrange the series in the data frame and plot again. Are there any changes compared to 
# before? If yes/no, why? 
df_var <- df_var[,c(1,3,2)] 
estMdl <- VAR(df_var[,2:3], p = 2, type="const")

# Plot IRs
irf <- irf(estMdl,n.ahead=10,ortho=FALSE,impulse=c("y2","y1"),response=c("y2","y1"))
plot(irf)

# Plot FEVD
fevd <- fevd(estMdl,n.ahead = 10)
par(mfrow=c(1,2),mar=c(2,2,2,2))
plot(fevd)

# Explanation
# res <- summary(estMdl)
# res$covres
# t(chol(res$covres))

```