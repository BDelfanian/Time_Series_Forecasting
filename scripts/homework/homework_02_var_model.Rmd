---
title: "Homework II – VAR Model"
authors: "Behrouz Delfanian, Panagiotis Valsamis"
affiliation: "University of Luxembourg"
description: "Practical Data Science for the Public Sector: Time Series Forecasting"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    number_sections: true
    code_folding: hide
  pdf_document:
    toc: true
    number_sections: true
params: {}
knitr:
  opts_chunk:
    message: false
    warning: false
---

```{r load libraries, include=FALSE}
library(ppcor)                            # for partial correlations
library(glmnet)                           # for lasso regression
library(lars)                             # for least angular regression model (similar to lasso)
library(readxl)
library(dplyr)
library(tibble)
library(purrr)
library(vars)
```

## Introduction to VAR Models

**Vector Autoregressive (VAR)** models are a fundamental class of **multivariate** time series models used to capture the **dynamic interrelationships** among multiple time-dependent variables. Unlike univariate models such as ARIMA, which model a single series in isolation, VAR models treat all variables as endogenous and explain each variable by its own past values and the past values of all other variables in the system.

Formally, a VAR model of order \(p\), denoted **VAR(\(p\))**, expresses each variable as a **linear combination** of \(p\) lags of itself and the other variables, plus an error term. This structure makes VAR models flexible and well suited for analyzing systems where variables mutually influence one another over time.

VAR models are widely used in time series analysis, particularly in economics and finance, to study macroeconomic dynamics such as the interactions between output, inflation, interest rates, and employment. They are also applied in fields like energy forecasting, environmental studies, and public policy analysis, where **multiple correlated time series evolve jointly**.

Key advantages of VAR models include their relatively simple estimation using ordinary least squares, their ability to capture rich dynamic dependencies, and the availability of well-established diagnostic and interpretive tools. These tools include **impulse response functions**, which trace the effect of a shock in one variable on others over time, and forecast error variance decompositions, which quantify the contribution of each shock to forecast uncertainty.

In forecasting applications, VAR models often outperform univariate approaches when additional series contain predictive information. However, they **require careful lag selection**, **stationarity of the involved series**, and attention to model stability to ensure reliable inference and forecasts.

### Mathematical Formulation

Let \( \mathbf{y}_t = (y_{1t}, y_{2t}, \dots, y_{kt})^\top \) be a \(k \times 1\) vector of **stationary time series** observed at time \(t\).
A Vector Autoregressive model of order \(p\), denoted VAR(\(p\)), is defined as

\[
\mathbf{y}_t
=
\mathbf{c}
+
\mathbf{A}_1 \mathbf{y}_{t-1}
+
\mathbf{A}_2 \mathbf{y}_{t-2}
+
\cdots
+
\mathbf{A}_p \mathbf{y}_{t-p}
+
\boldsymbol{\varepsilon}_t,
\]

where:

- \( \mathbf{c} \) is a \(k \times 1\) vector of intercept terms,
- \( \mathbf{A}_i \) for \( i = 1, \dots, p \) are \(k \times k\) coefficient matrices capturing lagged relationships,
- \( \boldsymbol{\varepsilon}_t \) is a \(k \times 1\) vector of error terms.

The error process \( \boldsymbol{\varepsilon}_t \) is typically assumed to satisfy

\[
\mathbb{E}[\boldsymbol{\varepsilon}_t] = \mathbf{0}, \quad
\mathbb{E}[\boldsymbol{\varepsilon}_t \boldsymbol{\varepsilon}_t^\top] = \boldsymbol{\Sigma}, \quad
\mathbb{E}[\boldsymbol{\varepsilon}_t \boldsymbol{\varepsilon}_{t-s}^\top] = \mathbf{0} \;\; \text{for } s \neq 0,
\]

where \( \boldsymbol{\Sigma} \) is a positive definite covariance matrix.

For illustration, a VAR(1) model with two variables \( (y_{1t}, y_{2t}) \) can be written explicitly as

\[
\begin{aligned}
y_{1t} &= c_1 + a_{11} y_{1,t-1} + a_{12} y_{2,t-1} + \varepsilon_{1t}, \\
y_{2t} &= c_2 + a_{21} y_{1,t-1} + a_{22} y_{2,t-1} + \varepsilon_{2t}.
\end{aligned}
\]

This formulation highlights that each variable depends on its own past and the past of all other variables in the system.

## Choose Additional Series

Selecting an additional time series is a crucial step in building a VAR model, as the inclusion of relevant variables can **improve both the explanatory power and the forecasting accuracy of the model**. In a VAR framework, all variables are treated as endogenous, meaning that each series can influence and be influenced by the others over time.

An additional series is helpful if it contains predictive information about the main series that is not already captured by its own past values. From a theory-based perspective, economic or contextual reasoning can justify why changes in one variable are expected to affect another. From a data-driven perspective, empirical relationships such as strong correlations or lead–lag patterns can indicate potential usefulness.

By incorporating a well-chosen additional series, the VAR model can better capture dynamic interactions, reduce omitted variable bias, and generate more accurate forecasts compared to a univariate approach.

### Theory-based Approach

#### General Rationale
In a VAR framework, the theory-based approach aims to include additional variables that are economically or financially linked to the target series. The goal is to **capture meaningful transmission channels**—such as income effects, financial market conditions, expectations, or macroeconomic cycles—that may influence the evolution of the variable of interest over time. A theoretically justified variable **increases interpretability** and **reduces the risk of spurious relationships**, while potentially **improving forecast performance**.

#### Target Series: Description
- **opcnet** — *Net issuance of assets or shares by investment funds (regulated collective investment funds, UCITS, domiciled in Luxembourg)*  
  This series measures net inflows into Luxembourg-domiciled investment funds and reflects investors’ portfolio allocation decisions, confidence, and financial market conditions.

#### Theory-based Candidate Series

1. **stoxx50_oe_base_q** — *Stock market index that tracks the stock-market performance of 50 leading blue-chip companies from 18 European countries.*  
   *Rationale:* Net fund issuance is closely linked to stock market performance. Rising equity markets tend to attract inflows into investment funds due to higher expected returns and positive wealth effects.

2. **vstoxx_m** — *stock prices- EZ stock price volatility index*  
   *Rationale:* Financial market uncertainty strongly affects investor behavior. Higher volatility typically leads to risk aversion and fund outflows, while stable conditions support net issuance.

3. **irt_lt_ea_oe_pc_q** — *Euro Area Interest rates, Long-term interest rates, average yields on government bonds issued by euro-area countries*  
   *Rationale:* Long-term interest rates influence portfolio allocation between bonds, equities, and investment funds. Changes in yields affect the relative attractiveness of fund investments.

4. **ecb_cissdu2z0z4fecss_cinidx** — *Euro Area, Leading Indicators, ECB, New Composite Indicator of Systemic Stress (CISS), Index*  
   *Rationale:* Systemic financial stress directly impacts capital flows into investment funds. Periods of high stress are associated with withdrawals and reduced issuance.

5. **clize** — *Composite Leading Indicator for the Euro Area by the OECD*  
   *Rationale:* Investment fund flows are forward-looking and may respond to expectations about future economic activity captured by leading indicators.

#### Summary
From a theoretical perspective, variables capturing **financial market performance**, **risk and uncertainty**, and **macro-financial conditions** are the most relevant for explaining net fund issuance. Among the available series, *equity market indices* and *financial stress measures* are particularly well aligned with the economic drivers of `opcnet`.


### Data-driven Approach

In the data-driven approach, additional series are selected based on their empirical relationship with the target variable. Specifically, the correlation coefficient between the `opcnet` series and each available candidate series is computed. Series exhibiting the highest absolute correlation with `opcnet` are considered the most relevant, as they display the **strongest linear association** and are therefore more likely to improve the forecasting performance of the VAR model.

#### Loading Helper Functions

The following code chunk automatically loads all custom helper functions required for the analysis. It scans the `helper_functions` directory for R script files (`.R`) and sources each of them into the current R session. As a result, all user-defined functions become available for subsequent computations without cluttering the document with repeated code. The use of `invisible()` ensures that neither the code nor its output appears in the final knitted report.

```{r calling helper functions, include=FALSE}
functions_path <- "helper_functions"                                            # functions path  
invisible(lapply(list.files(functions_path, pattern = "\\.R$",                  # source functions
                            full.names = TRUE), source))
```

#### Data Import

This code chunk imports the datasets used in the VAR analysis. The explanatory variables are loaded from the `data_x` sheet, while the target series (`opcnet`) is extracted from the `data_y`. In both cases, the date column is converted to a proper `Date` format to ensure correct time alignment.

```{r import data}
data_x <- read_excel("../../data/data_mod.xlsx", sheet = "data_x")

data_x$date <- as.Date(data_x$date)                                             # Converts first column to date-type 
start_date <- "2007-04-01"
end_date <- data_x$date[nrow(data_x)]

ind_start_x <- which(data_x$date == as.Date(start_date))                        # Cuts data at selected date
ind_end_x <- which(data_x$date == as.Date(end_date))
data_x <- data_x[ind_start_x:ind_end_x, ]

data_y <- read_excel("../../data/data_homework_01.xlsx", sheet = "data_y") |>
  select(date, opcnet) |> 
  tidyr::drop_na()

data_y$date <- as.Date(data_y$date) 

ind_start_y    <- which(data_y$date == as.Date(start_date))                     # Cuts data at selected date
ind_end_y      <- which(data_y$date == as.Date(end_date))
data_y         <- data_y[ind_start_y:ind_end_y, ]
```

The sample period is restricted to start on **2007-04-01** and end at the last available observation, ensuring consistency across datasets. Observations outside this common time window are removed by indexing on the date variable. Missing values in the target series are dropped to avoid inconsistencies in subsequent modeling steps.

As a result, both datasets are aligned in terms of time span and frequency, making them suitable for joint analysis and combination in a multivariate time series framework.

#### Data Preparation

The following code chunk constructs the final dataset used for VAR estimation and forecasting. First, the target series (`opcnet`) is appended to the explanatory dataset, creating a single data object that contains all variables aligned by time. The maximum lag length (`p_max`) and the forecast horizon (`k`) are then specified, and the name of the dependent variable is explicitly declared.

```{r data preparation}
data <- cbind(data_x, opcnet = data_y$opcnet)

p_max <- 3                                      # maximum lag to add
k <- 1                                          # forecast horizon (series will be lagged for direct forecasting at max k),
y_name = "opcnet"                               # Declare name of the dependent variable

y_xx  <- prepare_y_xx(data, k, p_max, y_name)   # aligns data for k-steps ahead direct forecasting, adds lags of y and X, optionaly drops data
```

The function `prepare_y_xx()`—sourced earlier from the `helper_functions` directory—is used to transform the raw data into a format suitable for direct multi-step-ahead forecasting. Inside this function, first differences of selected variables (including `opcnet`) are computed to help achieve stationarity. Optional variables can be removed if needed.

The function then aligns the data for a \(k\)-step-ahead forecast by shifting the dependent variable forward in time. Autoregressive lags up to order `p_max` are added for both the dependent variable and the explanatory variables. Any lags exceeding the specified maximum are removed to ensure a consistent model structure.

Finally, the lagged dependent and explanatory variables are merged into a single dataset, indexed by date. The resulting object contains the appropriately lagged and aligned variables required for estimating the VAR model and producing recursive forecasts.

#### Correlated Variables

This code chunk implements a data-driven procedure to identify candidate variables that are strongly related to the target series `opcnet`. Using the prepared dataset `y_xx`, the function `best_correlated()` computes pairwise correlation coefficients between `opcnet` and each available explanatory variable.

To ensure comparability, correlations are calculated only over the sample period where the dependent variable is observable, using complete cases to handle missing values. Variables are then ranked according to the absolute value of their correlation coefficients, and only those exceeding a predefined threshold are retained.

The output is a concise list of variables that exhibit a sufficiently strong linear relationship with `opcnet`, making them suitable candidates for inclusion in the VAR model from an empirical, data-driven perspective.

```{r correlated}
corr_threshold  <- .3                                                           # threshold for correlation coeff (drop less correlated variables)
names_include_1 <- best_correlated(y_xx, y_name, corr_threshold)                  # returns best variables

# Convert the 1-row data frame to a 2-column tibble
names_include_1_top10 <- tibble(Series = names(names_include_1)[1:10],
                                Value  = as.numeric(names_include_1[1, 1:10])
                                )

names_include_1_top10
```

#### Partially Correlated Variables

This step extends the data-driven selection by identifying variables that are **partially correlated** with the target series `opcnet`, after controlling for its own autoregressive dynamics. The objective is to detect explanatory variables that provide additional information beyond what is already captured by past values of the dependent variable.

First, all possible linear regression models of `opcnet` using its lagged values are estimated, and these models are ranked according to the corrected Akaike Information Criterion (AICc). The set of regressors associated with the best-ranked model represents the optimal autoregressive structure for `opcnet`.

Next, partial correlation coefficients are computed between `opcnet` and each remaining candidate variable, conditional on the selected autoregressive terms. This isolates the direct relationship between each candidate variable and `opcnet`, net of persistence effects. Variables with absolute partial correlations exceeding a predefined threshold (here 0.2) are retained.

The resulting list highlights variables that contribute independent explanatory power and are therefore strong candidates for inclusion in the VAR model from a refined, data-driven perspective.

```{r partially correlated}
corr_threshold    <- .2                                                         # threshold for partial correlation coeff (drop less correlated variables) 
reg_models_ranked <- rank_regression_models_by_ic(y_xx, y_name, "aicc")
best_ar           <- strsplit(reg_models_ranked$Regressors[1], ",\\s*")[[1]]
names_include_2   <- best_partially_correlated(y_xx, y_name, y_xx[best_ar], corr_threshold)  # returns best partially y-correlated variables

# Convert the 1-row data frame to a 2-column tibble
names_include_2_top10 <- tibble(Series = names(names_include_2)[1:10],
                                Value  = as.numeric(names_include_2[1, 1:10])
                                )

names_include_2_top10
```

#### LASSO Regression

This step applies a regularization-based, data-driven method to identify relevant predictors for `opcnet` using LASSO (Least Absolute Shrinkage and Selection Operator) regression. Unlike correlation-based methods, LASSO performs variable selection and shrinkage simultaneously, making it well suited for settings with many potentially correlated regressors.

The procedure first constructs a balanced dataset by removing variables that do not have observations at the chosen cut-off date, ensuring comparability across regressors. The dependent variable and candidate predictors are then combined into a matrix and restricted to complete cases.

A rolling time-series cross-validation scheme is used to select the penalty parameter governing the strength of regularization. Based on this optimal penalty, LASSO regressions are estimated across folds, and the union of variables with non-zero coefficients is retained.

The resulting set of variables represents those that contribute most strongly to explaining `opcnet` while controlling for overfitting, providing an additional, complementary perspective for data-driven variable selection.

```{r LASSO regression}
#cut_date = "2008-01-01"
cut_date = end_date
names_include_3 <- best_lasso_variables(y_xx, y_name , k, cut_date)

# Convert the 1-row data frame to a 2-column tibble
names_include_3_top10 <- tibble(Series = names(names_include_3)[1:10],
                                Value  = as.numeric(names_include_3[1, 1:10])
                                )

names_include_3_top10
```

#### LARS Regression

This step applies the Least Angle Regression (LARS) algorithm to identify relevant predictors for `opcnet`. LARS is a forward selection method closely related to LASSO that incrementally adds variables based on their explanatory power, making it particularly suitable for high-dimensional settings with correlated regressors.

The procedure first restricts the sample to observations on or after a specified cut-off date, ensuring a balanced dataset with no missing values across the dependent and explanatory variables. Variables with incomplete observations are removed to maintain comparability.

Optionally, the dimensionality of the predictor set can be reduced further by retaining only variables whose correlation with `opcnet` exceeds a predefined threshold. The LARS algorithm is then applied to the resulting dataset, and variables are ranked according to the order in which they enter the model.

The output provides a ranked list of predictors selected by LARS, highlighting variables that explain `opcnet` most effectively from a sequential, regression-based perspective.

```{r LARS regression}
cut_date = "2022-07-01"
#cut_date = end_date
names_include_4 <- best_lars_variables(y_xx, y_name, cut_date)

# Convert the 1-row data frame to a 2-column tibble
names_include_4_top10 <- tibble(Series = names(names_include_4)[1:10],
                                Value  = as.numeric(names_include_4[1, 1:10])
                                )

names_include_4_top10
```

The `cut_date = "2022-07-01"` is the latest date for which the code runs smoothly.

#### Stepwise Regression

This step employs a stepwise regression approach to identify relevant predictors for the target series `opcnet`. Stepwise regression is a sequential variable selection method that adds explanatory variables one at a time based on their incremental contribution to explaining the dependent variable.

The procedure begins by restricting the sample to observations on or after a specified cut-off date and removing any variables with missing values, resulting in a balanced dataset. This ensures that all candidate regressors are evaluated over a common time span.

Optionally, the dimensionality of the predictor set can be reduced by retaining only variables whose correlation with `opcnet` exceeds a predefined threshold. The stepwise selection is then implemented using the LARS algorithm with a stepwise selection rule, which determines the order in which variables enter the model.

The output is a ranked list of variables selected by the stepwise procedure, highlighting predictors that provide the strongest marginal explanatory power for `opcnet`.

```{r Stepwise regression}
cut_date = "2022-07-01"
#cut_date = end_date
names_include_5 <- best_stepwise_variables(y_xx, y_name, cut_date)

# Convert the 1-row data frame to a 2-column tibble
names_include_5_top10 <- tibble(Series = names(names_include_5)[1:10],
                                Value  = as.numeric(names_include_5[1, 1:10])
                                )

names_include_5_top10
```

Again, the `cut_date = "2022-07-01"` is the latest date for which the code runs smoothly.

#### Combining Results

This code identifies **the most consistently selected predictors** across the five variable selection methods. First, the top 10 variables from each method are combined into a single list. The frequency of each variable’s appearance is then counted, and the five variables that occur most frequently across all methods are extracted. The resulting tibble highlights the predictors that are most robustly chosen, providing a data-driven consensus for inclusion in the VAR model.

```{r conclusion}
# Combine all top10 tibbles into a list
top10_list <- list(
  names_include_1_top10,
  names_include_2_top10,
  names_include_3_top10,
  names_include_4_top10,
  names_include_5_top10
)

# Count frequency of each Series across all sets
top5_freq <- top10_list |>
  map_dfr(~ select(.x, Series)) |>   # keep only the Series column
  count(Series, sort = TRUE) |>      # count occurrences
  slice_head(n = 5)                  # take top 5

top5_freq
```

Here are the descriptions for the series highlighted in the analysis:

| Series Code           | Description                                                                |
| --------------------- | -------------------------------------------------------------------------- |
| cpi_i2015_inx_lu_q_sa | Luxembourg, Consumer Price Index, Total, Overall, Index, Seasonal adjusted |
| p_msn_fr_oe_base_q    | *No description provided in the source dataset!*                           |
| emp_de_oe_base_q      | DE, Employment, total                                                      |


The data-driven analysis indicates that `opcnet` (Net issuance of assets or shares by regulated Luxembourg investment funds, UCITS) is most strongly correlated with `cpi_i2015_inx_lu_q_sa` (Luxembourg, Consumer Price Index, Total, Overall, Index, Seasonally Adjusted). This is reasonable:

- `opcnet` reflects investment fund activity in Luxembourg, which could be sensitive to local inflation or price levels, captured by `cpi_i2015_inx_lu_q_sa`.

_ The other series, `emp_de_oe_base_q`, is an employment indicator for Germany, which is geographically and economically less directly connected to Luxembourg’s fund issuance; a lower correlation makes sense.

- The lack of description for `p_msn_fr_oe_base_q` limits interpretation, but its presence among the top correlated series does not invalidate the result—it just requires cautious reporting.


### Conclusion

Based on both theory and data, the selection of an additional series for the VAR model is motivated as follows.

The target series, **`opcnet`** (*Net issuance of assets or shares by Luxembourg-domiciled investment funds*), reflects investor behavior and is influenced by financial conditions, macroeconomic indicators, and expectations.  

- **Theory-based reasoning:** Economic theory suggests that net fund issuance responds to:  
  1. **Financial market performance** – e.g., equity indices (`stoxx50_oe_base_q`) indicate investor wealth and return expectations.  
  2. **Risk and uncertainty** – e.g., volatility indices (`vstoxx_m`) and systemic stress measures (`ecb_cissdu2z0z4fecss_cinidx`) reflect risk aversion affecting fund flows.  
  3. **Macro-financial conditions** – e.g., long-term interest rates (`irt_lt_ea_oe_pc_q`) and leading indicators (`clize`) provide information on economic trends that influence investment decisions.  

- **Data-driven evidence:** Empirical correlations highlight variables with strong associations with `opcnet`, particularly:  
  - **`cpi_i2015_inx_lu_q_sa`** – Luxembourg consumer price index, reflecting domestic inflation and wealth effects.  
  - **`emp_de_oe_base_q`** – German employment, capturing regional labor market conditions that may influence cross-border investment flows.  
  - **`p_msn_fr_oe_base_q`** – likely related to French market production or sales; although a precise description is unavailable, it empirically co-moves with `opcnet`.  

Considering both theoretical relevance and empirical correlation, a promising candidate series to include in the VAR model is:

- **`cpi_i2015_inx_lu_q_sa`** — *Luxembourg Consumer Price Index, total, seasonally adjusted*  

**Rationale:**  
This series is both theoretically plausible, as inflation affects investor behavior and fund allocations, and empirically relevant, as it consistently appears among the most correlated series with `opcnet`. Including it allows the VAR model to capture domestic macroeconomic dynamics that influence fund issuance, complementing other theory-based financial variables.

## VAR Estimation

We use the first 80% of the data to estimate the VAR model and determine the **optimal lag length** based on multiple criteria.  

### Split Data

```{r var-data-split}
# Define the two stationary series
trg_series = "opcnet"                                                           # Target series
exp_series = "L1.cpi_i2015_inx_lu_q_sa"                                         # Explanatory series
stationary_series <- y_xx[, c(trg_series, exp_series)]

# Split first 80% for estimation
n_obs <- nrow(stationary_series)
train_end <- floor(0.8 * n_obs)
train_data <- stationary_series[1:train_end, ]
train_data <- na.omit(train_data)
test_data  <- stationary_series[(train_end + 1):n_obs, ]
```

### Lag Selection

#### ACF and PACF

```{r lag, acf, pacf}
acf(train_data$opcnet, main = "ACF of opcnet")
pacf(train_data$opcnet, main = "PACF of opcnet")
acf(train_data$L1.cpi_i2015_inx_lu_q_sa, main = "ACF of L1.cpi_i2015_inx_lu_q_sa")
pacf(train_data$L1.cpi_i2015_inx_lu_q_sa, main = "PACF of L1.cpi_i2015_inx_lu_q_sa")
```

**Comment:**
The ACF and PACF plots suggest potential lag length based on where the autocorrelations decay or cut off. For example, spikes at lag 1 and 2 indicate that including 1–2 lags may capture the main autocorrelation structure.

#### Information Criteria

```{r info criteria}
VARselect(train_data, lag.max = 8, type = "const")$selection
```

**Comment:**
The table of AIC, HQ, SC and FPE values indicates the optimal lag length that minimizes each criterion. In this case, [X] lags are suggested by the BIC, which is often preferred for parsimony.

#### Significance of Coefficients

```{r significance of coeff}
var_model <- VAR(train_data, p = 1, type = "const")  # use lag from info criteria
summary(var_model)
```

**Comment:**
Most coefficients are statistically significant at conventional levels, confirming that the chosen lags capture meaningful relationships between the series. Non-significant coefficients may indicate overfitting or the need to reduce the lag length.

#### Residual Autocorrelation Tests

```{r residual autocorr tests}
serial.test(var_model, lags.pt = 10, type = "PT.asymptotic")
```

**Comment:**
The Ljung-Box test on residuals shows whether autocorrelation remains. P-values above 0.05 indicate no significant autocorrelation, suggesting the model adequately captures the dynamics.

#### Stability Checks

```{r stability checks}
stability(var_model, type = "OLS-CUSUM")
```

**Comment:**
The stability plot confirms that all roots lie within the unit circle, indicating that the VAR model is stable and suitable for forecasting. If roots were outside, model re-specification would be necessary.

