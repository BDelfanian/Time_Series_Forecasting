---
title: "VAR Model"
authors: "Behrouz Delfanian, Panagiotis Valsamis"
affiliation: "University of Luxembourg"
description: "Practical Data Science for the Public Sector: Time Series Forecasting"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    number_sections: true
    code_folding: hide
  pdf_document:
    toc: true
    number_sections: true
params: {}
knitr:
  opts_chunk:
    message: false
    warning: false
---

```{r load libraries, include=FALSE}
library(ppcor)                            # for partial correlations
library(glmnet)                           # for lasso regression
library(lars)                             # for least angular regression model (similar to lasso)
library(readxl)
library(dplyr)
library(tibble)
library(purrr)
library(vars)
library(forecast)
library(ggplot2)
```

## Introduction to VAR Models

**Vector Autoregressive (VAR)** models are a fundamental class of **multivariate** time series models used to capture the **dynamic interrelationships** among multiple time-dependent variables. Unlike univariate models such as ARIMA, which model a single series in isolation, VAR models treat all variables as endogenous and explain each variable by its own past values and the past values of all other variables in the system.

Formally, a VAR model of order \(p\), denoted **VAR(\(p\))**, expresses each variable as a **linear combination** of \(p\) lags of itself and the other variables, plus an error term. This structure makes VAR models flexible and well suited for analyzing systems where variables mutually influence one another over time.

VAR models are widely used in time series analysis, particularly in economics and finance, to study macroeconomic dynamics such as the interactions between output, inflation, interest rates, and employment. They are also applied in fields like energy forecasting, environmental studies, and public policy analysis, where **multiple correlated time series evolve jointly**.

Key advantages of VAR models include their relatively simple estimation using ordinary least squares, their ability to capture rich dynamic dependencies, and the availability of well-established diagnostic and interpretive tools. These tools include **impulse response functions**, which trace the effect of a shock in one variable on others over time, and forecast error variance decompositions, which quantify the contribution of each shock to forecast uncertainty.

In forecasting applications, VAR models often outperform univariate approaches when additional series contain predictive information. However, they **require careful lag selection**, **stationarity of the involved series**, and attention to model stability to ensure reliable inference and forecasts.

### Mathematical Formulation

Let \( \mathbf{y}_t = (y_{1t}, y_{2t}, \dots, y_{kt})^\top \) be a \(k \times 1\) vector of **stationary time series** observed at time \(t\).
A Vector Autoregressive model of order \(p\), denoted VAR(\(p\)), is defined as

\[
\mathbf{y}_t
=
\mathbf{c}
+
\mathbf{A}_1 \mathbf{y}_{t-1}
+
\mathbf{A}_2 \mathbf{y}_{t-2}
+
\cdots
+
\mathbf{A}_p \mathbf{y}_{t-p}
+
\boldsymbol{\varepsilon}_t,
\]

where:

- \( \mathbf{c} \) is a \(k \times 1\) vector of intercept terms,
- \( \mathbf{A}_i \) for \( i = 1, \dots, p \) are \(k \times k\) coefficient matrices capturing lagged relationships,
- \( \boldsymbol{\varepsilon}_t \) is a \(k \times 1\) vector of error terms.

The error process \( \boldsymbol{\varepsilon}_t \) is typically assumed to satisfy

\[
\mathbb{E}[\boldsymbol{\varepsilon}_t] = \mathbf{0}, \quad
\mathbb{E}[\boldsymbol{\varepsilon}_t \boldsymbol{\varepsilon}_t^\top] = \boldsymbol{\Sigma}, \quad
\mathbb{E}[\boldsymbol{\varepsilon}_t \boldsymbol{\varepsilon}_{t-s}^\top] = \mathbf{0} \;\; \text{for } s \neq 0,
\]

where \( \boldsymbol{\Sigma} \) is a positive definite covariance matrix.

For illustration, a VAR(1) model with two variables \( (y_{1t}, y_{2t}) \) can be written explicitly as

\[
\begin{aligned}
y_{1t} &= c_1 + a_{11} y_{1,t-1} + a_{12} y_{2,t-1} + \varepsilon_{1t}, \\
y_{2t} &= c_2 + a_{21} y_{1,t-1} + a_{22} y_{2,t-1} + \varepsilon_{2t}.
\end{aligned}
\]

This formulation highlights that each variable depends on its own past and the past of all other variables in the system.

## Choose Additional Series

Selecting an additional time series is a crucial step in building a VAR model, as the inclusion of relevant variables can **improve both the explanatory power and the forecasting accuracy of the model**. In a VAR framework, all variables are treated as endogenous, meaning that each series can influence and be influenced by the others over time.

An additional series is helpful if it contains predictive information about the main series that is not already captured by its own past values. From a theory-based perspective, economic or contextual reasoning can justify why changes in one variable are expected to affect another. From a data-driven perspective, empirical relationships such as strong correlations or lead–lag patterns can indicate potential usefulness.

By incorporating a well-chosen additional series, the VAR model can better capture dynamic interactions, reduce omitted variable bias, and generate more accurate forecasts compared to a univariate approach.

### Theory-based Approach

#### General Rationale
In a VAR framework, the theory-based approach aims to include additional variables that are economically or financially linked to the target series. The goal is to **capture meaningful transmission channels**—such as income effects, financial market conditions, expectations, or macroeconomic cycles—that may influence the evolution of the variable of interest over time. A theoretically justified variable **increases interpretability** and **reduces the risk of spurious relationships**, while potentially **improving forecast performance**.

#### Target Series: Description
- **opcnet** — *Net issuance of assets or shares by investment funds (regulated collective investment funds, UCITS, domiciled in Luxembourg)*  
  This series measures net inflows into Luxembourg-domiciled investment funds and reflects investors’ portfolio allocation decisions, confidence, and financial market conditions.

#### Theory-based Candidate Series

1. **stoxx50_oe_base_q** — *Stock market index that tracks the stock-market performance of 50 leading blue-chip companies from 18 European countries.*  
   *Rationale:* Net fund issuance is closely linked to stock market performance. Rising equity markets tend to attract inflows into investment funds due to higher expected returns and positive wealth effects.

2. **vstoxx_m** — *stock prices- EZ stock price volatility index*  
   *Rationale:* Financial market uncertainty strongly affects investor behavior. Higher volatility typically leads to risk aversion and fund outflows, while stable conditions support net issuance.

3. **irt_lt_ea_oe_pc_q** — *Euro Area Interest rates, Long-term interest rates, average yields on government bonds issued by euro-area countries*  
   *Rationale:* Long-term interest rates influence portfolio allocation between bonds, equities, and investment funds. Changes in yields affect the relative attractiveness of fund investments.

4. **ecb_cissdu2z0z4fecss_cinidx** — *Euro Area, Leading Indicators, ECB, New Composite Indicator of Systemic Stress (CISS), Index*  
   *Rationale:* Systemic financial stress directly impacts capital flows into investment funds. Periods of high stress are associated with withdrawals and reduced issuance.

5. **clize** — *Composite Leading Indicator for the Euro Area by the OECD*  
   *Rationale:* Investment fund flows are forward-looking and may respond to expectations about future economic activity captured by leading indicators.

#### Summary
From a theoretical perspective, variables capturing **financial market performance**, **risk and uncertainty**, and **macro-financial conditions** are the most relevant for explaining net fund issuance. Among the available series, *equity market indices* and *financial stress measures* are particularly well aligned with the economic drivers of `opcnet`.


### Data-driven Approach

In the data-driven approach, additional series are selected based on their empirical relationship with the target variable. Specifically, the correlation coefficient between the `opcnet` series and each available candidate series is computed. Series exhibiting the highest absolute correlation with `opcnet` are considered the most relevant, as they display the **strongest linear association** and are therefore more likely to improve the forecasting performance of the VAR model.

#### Loading Helper Functions

The following code chunk automatically loads all custom helper functions required for the analysis. It scans the `helper_functions` directory for R script files (`.R`) and sources each of them into the current R session. As a result, all user-defined functions become available for subsequent computations without cluttering the document with repeated code. The use of `invisible()` ensures that neither the code nor its output appears in the final knitted report.

```{r calling helper functions, include=FALSE}
functions_path <- "helper_functions"                                            # functions path  
invisible(lapply(list.files(functions_path, pattern = "\\.R$",                  # source functions
                            full.names = TRUE), source))
```

#### Data Import

This code chunk imports the datasets used in the VAR analysis. The explanatory variables are loaded from the `data_x` sheet, while the target series (`opcnet`) is extracted from the `data_y`. In both cases, the date column is converted to a proper `Date` format to ensure correct time alignment.

```{r import data}
data_x <- read_excel("../../data/data_mod.xlsx", sheet = "data_x")

data_x$date <- as.Date(data_x$date)                                             # Converts first column to date-type 
start_date <- "2007-04-01"
end_date <- data_x$date[nrow(data_x)]

ind_start_x <- which(data_x$date == as.Date(start_date))                        # Cuts data at selected date
ind_end_x <- which(data_x$date == as.Date(end_date))
data_x <- data_x[ind_start_x:ind_end_x, ]

data_y <- read_excel("../../data/data_homework.xlsx", sheet = "data_y") |>
  select(date, opcnet) |> 
  tidyr::drop_na()

data_y$date <- as.Date(data_y$date) 

ind_start_y    <- which(data_y$date == as.Date(start_date))                     # Cuts data at selected date
ind_end_y      <- which(data_y$date == as.Date(end_date))
data_y         <- data_y[ind_start_y:ind_end_y, ]
```

The sample period is restricted to start on **2007-04-01** and end at the last available observation, ensuring consistency across datasets. Observations outside this common time window are removed by indexing on the date variable. Missing values in the target series are dropped to avoid inconsistencies in subsequent modeling steps.

As a result, both datasets are aligned in terms of time span and frequency, making them suitable for joint analysis and combination in a multivariate time series framework.

#### Data Preparation

The following code chunk constructs the final dataset used for VAR estimation and forecasting. First, the target series (`opcnet`) is appended to the explanatory dataset, creating a single data object that contains all variables aligned by time. The maximum lag length (`p_max`) and the forecast horizon (`k`) are then specified, and the name of the dependent variable is explicitly declared.

```{r data preparation}
data <- cbind(data_x, opcnet = data_y$opcnet)

p_max <- 3                                      # maximum lag to add
k <- 1                                          # forecast horizon (series will be lagged for direct forecasting at max k),
y_name = "opcnet"                               # Declare name of the dependent variable

y_xx  <- prepare_y_xx(data, k, p_max, y_name)   # aligns data for k-steps ahead direct forecasting, adds lags of y and X, optionaly drops data
```

The function `prepare_y_xx()`—sourced earlier from the `helper_functions` directory—is used to transform the raw data into a format suitable for direct multi-step-ahead forecasting. Inside this function, first differences of selected variables (including `opcnet`) are computed to help achieve stationarity. Optional variables can be removed if needed.

The function then aligns the data for a \(k\)-step-ahead forecast by shifting the dependent variable forward in time. Autoregressive lags up to order `p_max` are added for both the dependent variable and the explanatory variables. Any lags exceeding the specified maximum are removed to ensure a consistent model structure.

Finally, the lagged dependent and explanatory variables are merged into a single dataset, indexed by date. The resulting object contains the appropriately lagged and aligned variables required for estimating the VAR model and producing recursive forecasts.

#### Correlated Variables

This code chunk implements a data-driven procedure to identify candidate variables that are strongly related to the target series `opcnet`. Using the prepared dataset `y_xx`, the function `best_correlated()` computes pairwise correlation coefficients between `opcnet` and each available explanatory variable.

To ensure comparability, correlations are calculated only over the sample period where the dependent variable is observable, using complete cases to handle missing values. Variables are then ranked according to the absolute value of their correlation coefficients, and only those exceeding a predefined threshold are retained.

The output is a concise list of variables that exhibit a sufficiently strong linear relationship with `opcnet`, making them suitable candidates for inclusion in the VAR model from an empirical, data-driven perspective.

```{r correlated}
corr_threshold  <- .3                                                           # threshold for correlation coeff (drop less correlated variables)
names_include_1 <- best_correlated(y_xx, y_name, corr_threshold)                  # returns best variables

# Convert the 1-row data frame to a 2-column tibble
names_include_1_top10 <- tibble(Series = names(names_include_1)[1:10],
                                Value  = as.numeric(names_include_1[1, 1:10])
                                )

names_include_1_top10
```

#### Partially Correlated Variables

This step extends the data-driven selection by identifying variables that are **partially correlated** with the target series `opcnet`, after controlling for its own autoregressive dynamics. The objective is to detect explanatory variables that provide additional information beyond what is already captured by past values of the dependent variable.

First, all possible linear regression models of `opcnet` using its lagged values are estimated, and these models are ranked according to the corrected Akaike Information Criterion (AICc). The set of regressors associated with the best-ranked model represents the optimal autoregressive structure for `opcnet`.

Next, partial correlation coefficients are computed between `opcnet` and each remaining candidate variable, conditional on the selected autoregressive terms. This isolates the direct relationship between each candidate variable and `opcnet`, net of persistence effects. Variables with absolute partial correlations exceeding a predefined threshold (here 0.2) are retained.

The resulting list highlights variables that contribute independent explanatory power and are therefore strong candidates for inclusion in the VAR model from a refined, data-driven perspective.

```{r partially correlated}
corr_threshold    <- .2                                                         # threshold for partial correlation coeff (drop less correlated variables) 
reg_models_ranked <- rank_regression_models_by_ic(y_xx, y_name, "aicc")
best_ar           <- strsplit(reg_models_ranked$Regressors[1], ",\\s*")[[1]]
names_include_2   <- best_partially_correlated(y_xx, y_name, y_xx[best_ar], corr_threshold)  # returns best partially y-correlated variables

# Convert the 1-row data frame to a 2-column tibble
names_include_2_top10 <- tibble(Series = names(names_include_2)[1:10],
                                Value  = as.numeric(names_include_2[1, 1:10])
                                )

names_include_2_top10
```

#### LASSO Regression

This step applies a regularization-based, data-driven method to identify relevant predictors for `opcnet` using LASSO (Least Absolute Shrinkage and Selection Operator) regression. Unlike correlation-based methods, LASSO performs variable selection and shrinkage simultaneously, making it well suited for settings with many potentially correlated regressors.

The procedure first constructs a balanced dataset by removing variables that do not have observations at the chosen cut-off date, ensuring comparability across regressors. The dependent variable and candidate predictors are then combined into a matrix and restricted to complete cases.

A rolling time-series cross-validation scheme is used to select the penalty parameter governing the strength of regularization. Based on this optimal penalty, LASSO regressions are estimated across folds, and the union of variables with non-zero coefficients is retained.

The resulting set of variables represents those that contribute most strongly to explaining `opcnet` while controlling for overfitting, providing an additional, complementary perspective for data-driven variable selection.

```{r LASSO regression}
#cut_date = "2008-01-01"
cut_date = end_date
names_include_3 <- best_lasso_variables(y_xx, y_name , k, cut_date)

# Convert the 1-row data frame to a 2-column tibble
names_include_3_top10 <- tibble(Series = names(names_include_3)[1:10],
                                Value  = as.numeric(names_include_3[1, 1:10])
                                )

names_include_3_top10
```

#### LARS Regression

This step applies the Least Angle Regression (LARS) algorithm to identify relevant predictors for `opcnet`. LARS is a forward selection method closely related to LASSO that incrementally adds variables based on their explanatory power, making it particularly suitable for high-dimensional settings with correlated regressors.

The procedure first restricts the sample to observations on or after a specified cut-off date, ensuring a balanced dataset with no missing values across the dependent and explanatory variables. Variables with incomplete observations are removed to maintain comparability.

Optionally, the dimensionality of the predictor set can be reduced further by retaining only variables whose correlation with `opcnet` exceeds a predefined threshold. The LARS algorithm is then applied to the resulting dataset, and variables are ranked according to the order in which they enter the model.

The output provides a ranked list of predictors selected by LARS, highlighting variables that explain `opcnet` most effectively from a sequential, regression-based perspective.

```{r LARS regression}
cut_date = "2022-07-01"
#cut_date = end_date
names_include_4 <- best_lars_variables(y_xx, y_name, cut_date)

# Convert the 1-row data frame to a 2-column tibble
names_include_4_top10 <- tibble(Series = names(names_include_4)[1:10],
                                Value  = as.numeric(names_include_4[1, 1:10])
                                )

names_include_4_top10
```

The `cut_date = "2022-07-01"` is the latest date for which the code runs smoothly.

#### Stepwise Regression

This step employs a stepwise regression approach to identify relevant predictors for the target series `opcnet`. Stepwise regression is a sequential variable selection method that adds explanatory variables one at a time based on their incremental contribution to explaining the dependent variable.

The procedure begins by restricting the sample to observations on or after a specified cut-off date and removing any variables with missing values, resulting in a balanced dataset. This ensures that all candidate regressors are evaluated over a common time span.

Optionally, the dimensionality of the predictor set can be reduced by retaining only variables whose correlation with `opcnet` exceeds a predefined threshold. The stepwise selection is then implemented using the LARS algorithm with a stepwise selection rule, which determines the order in which variables enter the model.

The output is a ranked list of variables selected by the stepwise procedure, highlighting predictors that provide the strongest marginal explanatory power for `opcnet`.

```{r Stepwise regression}
cut_date = "2022-07-01"
#cut_date = end_date
names_include_5 <- best_stepwise_variables(y_xx, y_name, cut_date)

# Convert the 1-row data frame to a 2-column tibble
names_include_5_top10 <- tibble(Series = names(names_include_5)[1:10],
                                Value  = as.numeric(names_include_5[1, 1:10])
                                )

names_include_5_top10
```

Again, the `cut_date = "2022-07-01"` is the latest date for which the code runs smoothly.

#### Combining Results

This code identifies **the most consistently selected predictors** across the five variable selection methods. First, the top 10 variables from each method are combined into a single list. The frequency of each variable’s appearance is then counted, and the five variables that occur most frequently across all methods are extracted. The resulting tibble highlights the predictors that are most robustly chosen, providing a data-driven consensus for inclusion in the VAR model.

```{r conclusion}
# Combine all top10 tibbles into a list
top10_list <- list(
  names_include_1_top10,
  names_include_2_top10,
  names_include_3_top10,
  names_include_4_top10,
  names_include_5_top10
)

# Count frequency of each Series across all sets
top5_freq <- top10_list |>
  map_dfr(~ select(.x, Series)) |>   # keep only the Series column
  count(Series, sort = TRUE) |>      # count occurrences
  slice_head(n = 5)                  # take top 5

top5_freq
```

Here are the descriptions for the series highlighted in the analysis:

| Series Code           | Description                                                                |
| --------------------- | -------------------------------------------------------------------------- |
| cpi_i2015_inx_lu_q_sa | Luxembourg, Consumer Price Index, Total, Overall, Index, Seasonal adjusted |
| p_msn_fr_oe_base_q    | *No description provided in the source dataset!*                           |
| emp_de_oe_base_q      | DE, Employment, total                                                      |


The data-driven analysis indicates that `opcnet` (Net issuance of assets or shares by investment funds) is most strongly correlated with `cpi_i2015_inx_lu_q_sa` (Luxembourg, Consumer Price Index, Total, Overall, Index, Seasonal adjusted). This is reasonable:

- `opcnet` reflects investment fund activity in Luxembourg, which could be sensitive to local inflation or price levels, captured by `cpi_i2015_inx_lu_q_sa`.

_ The other series, `emp_de_oe_base_q`, is an employment indicator for Germany, which is geographically and economically less directly connected to Luxembourg’s fund issuance; a lower correlation makes sense.

- The lack of description for `p_msn_fr_oe_base_q` limits interpretation, but its presence among the top correlated series does not invalidate the result—it just requires cautious reporting.


### Conclusion

Based on both theory and data, the selection of an additional series for the VAR model is motivated as follows.

The target series, **`opcnet`** (*Net issuance of assets or shares by investment funds*), reflects investor behavior and is influenced by financial conditions, macroeconomic indicators, and expectations.  

- **Theory-based reasoning:** Economic theory suggests that net fund issuance responds to:  
  1. **Financial market performance** – e.g., equity indices (`stoxx50_oe_base_q`) indicate investor wealth and return expectations.  
  2. **Risk and uncertainty** – e.g., volatility indices (`vstoxx_m`) and systemic stress measures (`ecb_cissdu2z0z4fecss_cinidx`) reflect risk aversion affecting fund flows.  
  3. **Macro-financial conditions** – e.g., long-term interest rates (`irt_lt_ea_oe_pc_q`) and leading indicators (`clize`) provide information on economic trends that influence investment decisions.  

- **Data-driven evidence:** Empirical correlations highlight variables with strong associations with `opcnet`, particularly:  
  - **`cpi_i2015_inx_lu_q_sa`** – Luxembourg consumer price index, reflecting domestic inflation and wealth effects.  
  - **`emp_de_oe_base_q`** – German employment, capturing regional labor market conditions that may influence cross-border investment flows.  
  - **`p_msn_fr_oe_base_q`** – likely related to French market production or sales; although a precise description is unavailable, it empirically co-moves with `opcnet`.  

Considering both theoretical relevance and empirical correlation, a promising candidate series to include in the VAR model is:

- **`cpi_i2015_inx_lu_q_sa`** — *Luxembourg, Consumer Price Index, Total, Overall, Index, Seasonal adjusted*  

**Rationale:**  
This series is both theoretically plausible, as inflation affects investor behavior and fund allocations, and empirically relevant, as it consistently appears among the most correlated series with `opcnet`. Including it allows the VAR model to capture domestic macroeconomic dynamics that influence fund issuance, complementing other theory-based financial variables.

## VAR Estimation

We use the first 80% of the data to estimate the VAR model and determine the **optimal lag length** based on multiple criteria.  

### Split Data

```{r var-data-split}
# Define the two stationary series
trg_series = "opcnet"                                                           # Target series
exp_series = "L1.cpi_i2015_inx_lu_q_sa"                                         # Explanatory series
stationary_series <- y_xx[, c("date", trg_series, exp_series)]
stationary_series <- na.omit(stationary_series)

# Split first 80% for estimation
n_total <- nrow(stationary_series)
n_train <- floor(0.8 * n_total)
train_data <- stationary_series[1:n_train, ]
#train_data <- na.omit(train_data)
test_data  <- stationary_series[(n_train + 1):n_total, ]
```

### Lag Selection

Several combinations of explanatory series (`L1.cpi_i2015_inx_lu_q_sa`, `L2.cpi_i2015_inx_lu_q_sa`, `L2.p_msn_fr_oe_base_q`, `L3.p_msn_fr_oe_base_q`, `L2.trb1g_vj_clv_qna_sa`) and lag orders (`1`, `2`, `3`) were tested.  

The final specification is selected based on a logical trade-off between information criteria, coefficient significance, residual diagnostics, and model stability.

#### ACF and PACF

```{r lag, acf, pacf}
acf(train_data[[2]], main = paste("ACF of", trg_series))
pacf(train_data[[2]], main = paste("ACF of", trg_series))
acf(train_data[[3]], main = paste("ACF of", exp_series))
pacf(train_data[[3]], main = paste("ACF of", exp_series))
```

**Comment:**  
The ACF and PACF plots for both series show a rapid decay after the first lag, with no strong evidence of long-memory behavior. This suggests that a low-order VAR (one or two lags) is sufficient to capture the dynamic dependence structure.

#### Information Criteria

```{r info criteria}
VARselect(train_data[,2:3], lag.max = 8, type = "const")$selection
```

**Comment:**  
All information criteria (AIC, HQ, SC, and FPE) unanimously select one lag. Given the relatively small sample size, the agreement across criteria strongly supports a parsimonious (fewer parameters, easier to interpret) VAR(1) specification.

#### Significance of Coefficients

```{r significance of coeff}
var_model <- VAR(train_data[,2:3], p = 1, type = "const")  # use lag from info criteria
summary(var_model)
```

**Comment:**  
In the VAR(1) model, the lag of the explanatory CPI series is statistically significant in the `opcnet` equation, indicating meaningful predictive content. When higher-order lags were included, additional coefficients were largely insignificant, suggesting that VAR(2) introduces unnecessary complexity without substantial gains in explanatory power.

#### Residual Autocorrelation Tests

```{r residual autocorr tests}
serial.test(var_model, lags.pt = 10, type = "PT.asymptotic")
```

**Comment:**  
The Portmanteau test fails to reject the null hypothesis of no residual autocorrelation (p-value > 0.05). This indicates that the VAR(1) specification adequately captures the serial dependence in the data, and higher lag orders are not required to whiten the residuals.

#### Stability Checks

```{r stability checks}
stability_test <- stability(var_model, type = "OLS-CUSUM")
plot(stability_test)
```

**OLS–CUSUM Stability Test**

The OLS–CUSUM test assesses the stability of the VAR coefficients over time.  
The plot shows the cumulative sum of recursive residuals together with 5% critical bounds.

**Comment:**  
The CUSUM path remains entirely within the confidence bands throughout the sample period. Therefore, we do not reject the null hypothesis of parameter stability, confirming that the estimated VAR(1) model is structurally stable.

### Final Model Choice

Based on the combined evidence, the final model is specified as a **VAR(1)** with:

- target series: `opcnet`,

- explanatory series: `L1.cpi_i2015_inx_lu_q_sa`.

This specification represents a balanced trade-off between goodness of fit, parsimony, statistical significance, well-behaved residuals, and coefficient stability, making it suitable for inference and forecasting.

### Economic Interpretation of the VAR Specification

The selected VAR(1) model linking `opcnet` (Net issuance of assets or shares by investment funds) and the lagged CPI (`cpi_i2015_inx_lu_q_sa`) is economically well-founded.

The Consumer Price Index reflects overall inflationary conditions, which influence household savings behavior, investment decisions, and expectations regarding monetary policy. These effects do not materialize instantaneously but operate with delays due to information processing, portfolio reallocation, and institutional frictions. Given the quarterly frequency of the data, a one-quarter lag is therefore economically appropriate.

The estimated negative effect of lagged CPI on net fund issuance is consistent with the notion that higher inflation reduces real returns and may lead to tighter financial conditions, thereby dampening demand for investment fund shares. Consequently, the chosen explanatory variable and lag structure capture a plausible macro–financial transmission mechanism.

## One-Step-Ahead Forecast

To evaluate the out-of-sample forecasting performance, we conduct **one-step-ahead recursive forecasts** over the last 20% of the sample.

At each forecast origin (t), the models are re-estimated using all available observations up to that point:

- First, the models are estimated on observations 1,...,T (with T corresponding to 80% of the sample).
- A one-step-ahead forecast for T+1 is produced.
- The sample is then expanded by one observation, and the process is repeated until the end of the dataset.

Two competing models are considered:

- A **VAR(1)** model including `opcnet` and `L1.cpi_i2015_inx_lu_q_sa`
- A univariate **ARIMA(1,0,0)** model for `opcnet`

This recursive strategy ensures that both models are evaluated under identical forecasting conditions and avoids look-ahead bias.

```{r var, arima forecase}
# Initialize vector for VAR forecasts
var_forecasts <- numeric(n_total - n_train)

# Initialize vector for ARIMA forecasts
arima_forecasts <- numeric(n_total - n_train)

for (t in 1:(n_total - n_train)) {
  # Recursive estimation
  var_fit <- VAR(stationary_series[1:(n_train + t - 1), 2:3], p = 1, type = "const")
  arima_fit <- Arima(stationary_series$opcnet[1:(n_train + t - 1)], order = c(1,0,0), include.mean = TRUE)
  
  # Forecast one step ahead
  var_pred <- predict(var_fit, n.ahead = 1)
  
  # Store forecast for opcnet
  var_forecasts[t] <- var_pred$fcst$opcnet[1, "fcst"]
  arima_forecasts[t] <- forecast::forecast(arima_fit, h = 1)$mean
}

# Actual values
actual <- stationary_series[(n_train + 1):n_total, "opcnet"]

# Compute RMSE
var_rmse <- sqrt(mean((var_forecasts - actual)^2))
arima_rmse <- sqrt(mean((arima_forecasts - actual)^2))
```


```{r forecast rmse}
cat("VAR(1) RMSE:", var_rmse, "\n")
cat("ARIMA(1,0,0) RMSE:", arima_rmse, "\n")
```

### Forecast Accuracy (RMSE)

The forecasting accuracy is assessed using the **Root Mean Squared Error (RMSE)** over the forecast evaluation period.

* **VAR(1) RMSE:** `r round(var_rmse, 3)`
* **ARIMA(1,0,0) RMSE:** `r round(arima_rmse, 3)`

**Interpretation:**  
The VAR(1) model achieves a lower RMSE than the ARIMA(1,0,0), indicating superior one-step-ahead forecasting performance. This suggests that incorporating information from inflation dynamics (CPI) improves forecasts of net fund issuance beyond what can be achieved using only the past values of `opcnet`.

### Visual Comparison of Forecasts

```{r plot forcasts}
# Create a time index for the forecast period
forecast_dates <- stationary_series$date[(n_train + 1):n_total]

# Combine into a data frame
forecast_df <- data.frame(
  date   = forecast_dates,
  Actual = actual,
  VAR    = var_forecasts,
  ARIMA  = arima_forecasts
)

ggplot(forecast_df, aes(x = date)) +
  # Actual series
  geom_line(aes(y = Actual, colour = "Actual"), linewidth = 1) +
  
  # VAR forecast
  geom_line(aes(y = VAR, colour = "VAR(1)"), linewidth = 1) +
  
  # ARIMA forecast
  geom_line(aes(y = ARIMA, colour = "ARIMA(1,0,0)"), linewidth = 1) +
  
  scale_colour_manual(
    name   = "",
    values = c(
      "Actual"        = "black",
      "VAR(1)"        = "blue",
      "ARIMA(1,0,0)"  = "red"
    )
  ) +
  
  labs(
    title = "One-Step-Ahead Forecasts: VAR(1) vs ARIMA(1,0,0)",
    x = "Date",
    y = "opcnet"
  ) +
  
  theme_minimal() +
  theme(
    legend.position = "top"
  )

```

The figure compares the actual values of `opcnet` with the one-step-ahead forecasts from both models:

- **Black line:** Actual series
- **Blue line:** VAR(1) forecasts
- **Red line:** ARIMA(1,0,0) forecasts

**Interpretation:**  
Both models capture the broad direction of movements in `opcnet`, but the VAR(1) forecasts track the actual series more closely, particularly during turning points. The ARIMA model appears smoother and reacts more slowly to changes, reflecting its reliance solely on the autoregressive structure of `opcnet`. In contrast, the VAR model benefits from additional information contained in CPI, allowing for more responsive and accurate predictions.

### Overall Conclusion

The results indicate that the **VAR(1) model dominates the ARIMA(1,0,0)** in terms of out-of-sample forecast accuracy. This finding is consistent with economic intuition: inflation dynamics influence investors’ real returns and portfolio decisions, making CPI a relevant explanatory variable for net issuance of investment fund shares. Consequently, modeling these interactions explicitly within a VAR framework leads to improved forecasting performance.


## Further Analysis

### Impulse Response Analysis

To further investigate the **dynamic interaction** between the variables, impulse response functions (IRFs) are computed from the estimated VAR(1) model. The analysis focuses on the response of the main variable, `opcnet`, to a one–standard-deviation shock in the explanatory variable, `L1.cpi_i2015_inx_lu_q_sa`.

Impulse responses are identified using a **Cholesky decomposition**, which requires an assumption on the contemporaneous ordering of the variables. Since the direction of **instantaneous causality** between inflation and investment fund flows is not obvious, IRFs are reported under both possible orderings to assess the robustness of the results.

#### Ordering 1: `opcnet` → CPI

In the first specification, the VAR is ordered as (`opcnet`, `L1.cpi_i2015_inx_lu_q_sa`).  
This implies that shocks to `opcnet` may affect CPI contemporaneously, while `opcnet` responds to CPI shocks only with a lag.

The VAR(1) model is estimated on the training sample (first 80% of observations), and the impulse response of `opcnet` to a shock in CPI is computed as follows:

```{r irf cpi to opcnet}
var_model_opcnet_first <- VAR(train_data[, 2:3], p = 1, type = "const")

irf_cpi_to_opcnet_rev <- irf(
  var_model_opcnet_first,
  impulse  = "L1.cpi_i2015_inx_lu_q_sa",
  response = "opcnet",
  n.ahead  = 12,
  boot     = TRUE
)

plot(irf_cpi_to_opcnet_rev)
```

#### Ordering 2: CPI → `opcnet`

In the alternative specification, the VAR is ordered as (`L1.cpi_i2015_inx_lu_q_sa`, `opcnet`).  
This ordering allows CPI shocks to affect `opcnet` contemporaneously, while CPI responds to `opcnet` only with a lag.

The impulse response is computed from the re-estimated VAR:

```{r irf trg to exp}
var_model_cpi_first <- VAR(train_data[, c(3,2)], p = 1, type = "const")

irf_cpi_to_opcnet <- irf(
var_model_cpi_first,
impulse  = "L1.cpi_i2015_inx_lu_q_sa",
response = "opcnet",
n.ahead  = 12,
boot     = TRUE
)

plot(irf_cpi_to_opcnet)
```

A positive shock to CPI generates an immediate negative response in `opcnet`, reaching its maximum magnitude within the first two quarters. The response gradually converges back to zero and becomes negligible after approximately 6–8 quarters.

The bootstrap confidence intervals indicate that the **short-run response is statistically meaningful**, while longer-horizon effects are not distinguishable from zero. This suggests that inflationary pressures may temporarily reduce net issuance by investment funds, potentially reflecting valuation effects, monetary tightening expectations, or portfolio rebalancing behavior.

#### Interpretation and Robustness

Across both orderings, the qualitative pattern of the impulse response is very similar. CPI shocks induce a short-lived negative effect on `opcnet`, which dissipates over time and does not lead to persistent deviations.

While the magnitude of the initial response differs slightly across orderings, the direction, timing, and transitory nature of the effect remain robust. This indicates that the main conclusions do not hinge critically on the assumed contemporaneous causal structure.

Overall, the impulse response analysis complements the forecasting results: although CPI shocks have a measurable short-run impact on `opcnet`, the transmission mechanism is limited in duration, consistent with the modest but non-negligible predictive gains observed in the VAR relative to the ARIMA benchmark.

### Granger Causality Tests

To formally assess whether past values of one variable contain predictive information about the other, Granger causality tests are conducted based on the estimated VAR(1) model. **Granger causality does not imply true economic causation**; rather, it evaluates whether lagged values of one series improve the prediction of another.

The tests are performed on the training sample using the same lag length as the VAR model.

#### Does CPI Granger-cause `opcnet`?

Null hypothesis: Lagged values of CPI do not Granger-cause `opcnet`.

```{r causality, cpi}
causality(var_model_cpi_first, cause = "L1.cpi_i2015_inx_lu_q_sa")
```

- F-Test = 6.0227, p-value = 0.0158

- **Interpretation**: Since the p-value < 0.05, we reject the null hypothesis that lagged CPI does not Granger-cause `opcnet`.

- Conclusion: Past values of CPI provide statistically significant information for predicting `opcnet`.

#### Does `opcnet` Granger-cause CPI?

Null hypothesis: Lagged values of `opcnet` do not Granger-cause CPI.

Unlike impulse response analysis, Granger causality tests are based solely on lagged coefficients and therefore do not depend on the contemporaneous ordering of variables. As a result, both causality directions are tested using the same estimated VAR(1) model.

```{r causality, opcnet}
causality(var_model_cpi_first, cause = "opcnet")
```

- F-Test = 1.2199, p-value = 0.2719

- **Interpretation**: The p-value > 0.05, so we fail to reject the null hypothesis.

- Conclusion: Past values of `opcnet` do not significantly improve the prediction of CPI.

#### Instantaneous Causality

- Test statistic = 0.40752, p-value = 0.5232 for both directions

- **Interpretation**: p-value > 0.05, so we fail to reject the null hypothesis.

- Conclusion: There is no evidence of instantaneous causality between CPI and `opcnet`; that is, they do not have a contemporaneous causal effect on each other.

#### Summary

- Causal effect from CPI to `opcnet` exists with a lag (Granger causality).

- Causal effect from `opcnet` to CPI does not exist.

- No instantaneous causal relationship is observed in either direction.

