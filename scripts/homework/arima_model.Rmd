---
title: "ARIMA Model"
authors: "Behrouz Delfanian, Panagiotis Valsamis"
affiliation: "University of Luxembourg"
description: "Practical Data Science for the Public Sector: Time Series Forecasting"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    number_sections: true
    code_folding: hide
  pdf_document:
    toc: true
    number_sections: true
params: {}
knitr:
  opts_chunk:
    message: false
    warning: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r load libraries, include=FALSE}
library(readxl)
library(ggplot2)
library(stats)
library(forecast)
library(ggpubr)
library(fpp3)
library(aTSA)
library(tseries)
library(urca)
```

The following code chunk loads data from a `.xlsx` file and creates a quarterly time series object `y` using the `date` and `opcnet` columns.\

It also checks if first element in `date` is already a `Date` or `POSIXt` object. If not (e.g., if it's a string or numeric from Excel), converts it to a `Date` object using the format `%d/%m/%Y` (day/month/year, e.g., "01/01/2007" becomes a `Date`).

```{r import data}
df <- read_excel("../../data/data_homework.xlsx", sheet = "data_y") |>
  select(date, opcnet) |> 
  tidyr::drop_na()

# extract the starting date
first_date <- df$date[1]
if (!inherits(first_date, c("Date", "POSIXt"))) {
  first_date <- as.Date(first_date, format = "%d/%m/%Y")
}

year <- as.numeric(format(first_date, "%Y"))
month <- as.numeric(format(first_date, "%m"))
quarter <- ceiling(month / 3)
# month 1 â†’ quarter 1; month 4 â†’ quarter 2

ts_data <- ts(df$opcnet, start = c(year, quarter), frequency = 4)
head(ts_data, 12)
```
The `ts()` function in R creates a time series object that is a numeric vector (or matrix for multivariate data) with a "ts" class, equipped with attributes like `tsp` (specifying start, end, and frequency), ensuring equispaced observations for time-based analysis. It supports specialized methods for plotting, forecasting, and statistical tests while maintaining chronological integrity during operations like subsetting or arithmetic.

# Stationarity and Transformations

## Plot the Series and its ACF & PACF
**ACF** (Autocorrelation Function) measures the correlation between a time series and its lagged versions, **helping identify patterns like seasonality or trends**.

**PACF** (Partial Autocorrelation Function) measures the correlation between the series and its lags **after removing the effects of intermediate lags**, useful for determining the order of autoregressive terms in models like ARIMA.

```{r plot_series_acf_pacf}
max_lag = 18

plot_ts <- ggplot(data.frame(x = time(ts_data), y = as.numeric(ts_data)), aes(x = x, y = y)) +
  geom_line(color = "black", size = 0.5) +
  labs(title = "opcnet Time Series", x = "Time (Years)", y = "opcnet ") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

plot_ac <- ggAcf(ts_data, lag.max = max_lag) + 
  labs(title = "ACF") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

plot_pac <- ggPacf(ts_data, lag.max = max_lag) + 
  labs(title = "PACF") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

ggarrange(plot_ts)
ggarrange(plot_ac)
ggarrange(plot_pac)

```

`Lag.max` specifies the maximum number of lags to display in ACF/PACF plots; for `N=75` observations, a common default (e.g., in R's `acf()` function) is `floor(10 * log10(75)) â‰ˆ 18`, which balances showing enough lags to **identify patterns** like AR/MA orders or seasonality while avoiding unreliable high-lag correlations due to reduced effective sample size. Keep it below `N/4 (â‰ˆ18.75)` to maintain statistical reliability.

**Interpretation**

- **Time Series Plot**:\
Stationary if mean and variance appear constant over time (no obvious trend, seasonality, or changing spread).\
Non-stationary if there's a trend (up/down drift), seasonality (repeating patterns), or heteroscedasticity (varying volatility).
- **ACF Plot**:\
Stationary if correlations **decay quickly to zero**.\
Non-stationary if they decay slowly or remain high over many lags (indicating persistence like unit root).
- **PACF Plot**:\
Stationary if partial correlations cut off after a few lags (suggesting finite AR order).\
Non-stationary if they show gradual decay or significance at many lags.

>Based on the explanations provided above, we may conclude that the `opcnet` time series appears to be *stationary*; however, further testing is required to confirm this.

## Test the Series for Stationarity

In time series analysis, assessing **stationarity** is crucial because many statistical models assume that the mean, variance, and autocorrelation structure of the series do not change over time. A common source of non-stationarity is the presence of a **unit root**, where shocks to the series have **permanent effects** and the series exhibits a stochastic trend rather than reverting to a fixed mean. The non-stationarity suggests **differencing** or **detrending** may be necessary to achieve stationarity before further modeling.

### ADF Test

The **Augmented Dickeyâ€“Fuller (ADF) test** is the most commonly used unit **root test**.

**Hypotheses**

- **Null hypothesis ($H_0$)**:\
  The series has a unit root â†’ **non-stationary**  
- **Alternative hypothesis ($H_1$)**:\
  The series is stationary (or trend-stationary)

**Test Equation Variants**\
1. No constant, no trend  
2. With constant (most common)  
   $$\Delta y_t = \alpha + (\rho-1)y_{t-1} + \sum_{i=1}^p \gamma_i \Delta y_{t-i} + \varepsilon_t$$
3. With constant + linear trend

**Performing the ADF test using `aTSA` library:**
```{r stationarity, ADF test, aTSA}
aTSA::adf.test(ts_data, nlag = 10)
```

`aTSA::adf.test()` provides three types of tests. For *net issuance of assets or shares by investment funds, `opcnet`*, the most suitable ADF specification is usually: **Type 2: with drift, no trend**

Reasoning:\

- Economic/financial context:
  - Net issuance data is typically **mean-reverting around a non-zero level** (funds issue or redeem shares around some typical size).
  - It rarely exhibits a strong deterministic linear trend over time (which would justify Type 3).

- Type 1 (no drift, no trend):
  - Assumes the series fluctuates around zero.  
  - Not realistic for financial flows, which often have a positive average.

- Type 3 (drift + trend):
  - Assumes the series has a deterministic upward or downward trend.  
  - Only appropriate if net issuance is systematically increasing or decreasing over time.  
  - Most fund issuance series fluctuate around a stable mean with random shocks.


**Performing the ADF test using `tseries` library:**
```{r stationarity, ADF test, tseries}
tseries::adf.test(ts_data)
```

`tseries::adf.test()` runs a single ADF test.  

By default:

- Includes a drift (intercept)
- No deterministic trend
- Automatically chooses the number of lags (can be set with `k`)

So in effect, it is **similar to Type 2 (drift, no trend)** of `aTSA::adf.test()`.

**Test Interpretation**

| ADF p-value       | Decision                          |
|-------------------|-----------------------------------|
| p â‰¤ 0.05          | Reject $H_0$ â†’ Stationary         |
| p > 0.05          | Fail to reject â†’ Unit root likely |

>The ADF test was applied under different deterministic specifications and lag lengths. While the null of a unit root is rejected for small to moderate lag orders when allowing for a drift, the result is sensitive to lag selection and disappears for larger lag lengths. Overall, **the evidence for stationarity is weak and not robust**.

To obtain a robust assessment, we applied the ADF test using `urca::ur.df()` with `type = "drift"` and lag selection based on `AIC`. 

```{r stationarity, ADF test, urca}
summary(urca::ur.df(ts_data, type = "drift", selectlags = "AIC"))
```

Using the Augmented Dickeyâ€“Fuller test with a drift, the null hypothesis of a unit root is rejected at the 1% significance level. **The series is stationary around a deterministic mean**.

Putting everything together:

- The unit root null hypothesis is rejected
- The drift term is significant
- No trend is included or needed

#### Final ADF Test Conclusion
>To assess the stationarity of the series, we first applied the Augmented Dickeyâ€“Fuller (ADF) test using both `aTSA::adf.test()` and `tseries::adf.test()`. With `aTSA`, we examined three deterministic specifications (no drift, drift only, and drift with trend) across a range of lag lengths. The results indicated that the rejection of the unit root was sensitive to lag choice: the null hypothesis was rejected for small to moderate lags when allowing for a drift, but the effect disappeared for larger lag lengths, suggesting that the evidence for stationarity was weak and not robust. To obtain a more reliable assessment, we then used `urca::ur.df()` with `type = "drift"` and lag selection based on AIC. This method accounts for finite-sample critical values and separates the test for a unit root from the significance of the drift term, providing a defensible conclusion. Using this approach, the series was found to be stationary around a non-zero mean (drift-stationary).


### KPSS Test

The **Kwiatkowskiâ€“Phillipsâ€“Schmidtâ€“Shin (KPSS) test** tests for **stationarity** around a deterministic trend or level â€” the opposite hypothesis of ADF/PP tests.

**Hypotheses (reversed compared to ADF/PP!)**

- **Null hypothesis ($H_0$)**:\
  The series is **stationary** (level or trend-stationary)  
- **Alternative hypothesis ($H_1$)**:\
  The series has a **unit root** (non-stationary)

**Test Types**

- `"mu"` â†’ tests stationarity around a **constant** (level-stationary)  
- `"tau"` â†’ tests stationarity around a **linear trend** (trend-stationary)

**Decision Rule**

| KPSS p-value       | Conclusion                          |
|--------------------|-------------------------------------|
| p > 0.05           | Fail to reject $H_0$ â†’ Stationary   |
| p â‰¤ 0.05           | Reject $H_0$ â†’ Non-stationary       |

**Recommended Joint Testing Strategy**

| ADF/PP Result       | KPSS Result         | Conclusion                          |
|---------------------|---------------------|-------------------------------------|
| Reject $H_0$ (stationary) | Fail to reject $H_0$ (stationary) | Strong evidence of stationarity     |
| Fail to reject (unit root) | Reject $H_0$ (non-stationary)     | Strong evidence of unit root     |
| Conflicting results |                                     | Inconclusive â†’ try differencing or other tests |


```{r stationarity, kpss test}
kpss.test(ts_data)
```

#### Final KPSS Test Conclusion

>The KPSS test fails to reject the null hypothesis of level stationarity (p-value â‰¥ 0.10). This indicates that there is no statistical evidence against the series being stationary around a constant. When combined with the ADF results, this provides consistent evidence that the series is stationary.

### PP Test
The **Phillipsâ€“Perron (PP) test** is a popular unit root test used to determine whether a time series is stationary or contains a unit root.

**Hypotheses**

- **Null hypothesis ($H_0$)**:\
  The series has a unit root â†’ *non-stationary*  
- **Alternative hypothesis ($H_1$)**:\
  The series is (trend-)stationary â†’ no unit root

**Key Advantages over ADF**

| Feature                        | ADF Test                          | PP Test                                      |
|-------------------------------|-----------------------------------|-----------------------------------------------|
| Correction for serial correlation | Parametric (adds lagged Î”y)      | Non-parametric (Neweyâ€“West HAC estimator)    |
| Robustness to heteroskedasticity | No                                | Yes                                           |
| Small-sample performance       | Generally better                  | Slightly lower power, but more robust         |

**Test Specifications (choose one)**

1. **No constant, no trend**  
2. **With constant (drift)** â†’ most common for economic series  
3. **With constant + linear trend** â†’ use when series shows clear trend

**Decision Rule**

| PP p-value       | Conclusion                          |
|--------------------|-------------------------------------|
| p-value < 0.05           | Reject $H_0$ â†’ series is stationary   |
| p-value > 0.05           | Fail to reject $H_0$ â†’ evidence of unit root (non-stationary)       |

```{r stationarity, pp test}
# Similar to Augmented DF test, performs worse than ADF in small samples
pp.test(ts_data)
```

#### Final PP Test Conclusion
>The Phillipsâ€“Perron test rejects the null hypothesis of a unit root at the 1% significance level (p â‰¤ 0.01). This provides evidence that the series is stationary. Since the PP test is robust to serial correlation and heteroskedasticity, this result supports and strengthens the conclusions obtained from the ADF and KPSS tests.

### Comparing All Tests

```{r stationarity, compare all}
stationary.test(ts_data, method = c("adf", "pp", "kpss"))
```

#### Final Stationarity Assessment

The stationarity of the series was assessed using multiple complementary tests, including the Augmented Dickeyâ€“Fuller (ADF), Phillipsâ€“Perron (PP), and KPSS tests.\

- The ADF test results, across different deterministic specifications and moderate lag lengths, generally reject the null hypothesis of a unit root when a drift is included.\
- The Phillipsâ€“Perron test also rejects the null hypothesis of a unit root at the 1% significance level, providing robust evidence of stationarity while accounting for serial correlation and heteroskedasticity.\
- In contrast, the KPSS test fails to reject the null hypothesis of level stationarity, indicating no evidence against stationarity around a constant.

Taken together, these results provide consistent evidence that the series is **stationary around a non-zero mean (drift-stationary)**. Although the ADF test shows some sensitivity to lag length in exploratory settings, the results obtained using `urca::ur.df()` with lag selection based on AIC, combined with the PP and KPSS tests, support the conclusion that the series does not contain a unit root.

#### Implications for ARIMA Modelling

Since the series is already stationary in levels, **no differencing is required** to achieve stationarity. Applying first differences or growth rates would therefore **risk overdifferencing** and removing meaningful dynamics from the data. Consequently, the original series will be used directly for ARIMA modelling. As a robustness check, stationarity tests may be re-applied after any transformation considered during model diagnostics.

```{r transformations}
diff_data <- diff(ts_data)
aTSA::adf.test(diff_data, nlag = 10)
```

**Interpretation:**\

>The ADF test applied to the first-differenced series strongly rejects the null hypothesis of a unit root across all deterministic specifications and lag lengths, with p-values uniformly below 1%. This indicates that **the differenced series is clearly stationary**. However, since previous ADF, PP, and KPSS tests already provided evidence that the original series is stationary in levels, **differencing is not required to achieve stationarity**. Consequently, applying first differences would likely result in overdifferencing. For ARIMA modelling, the series will therefore be used in levels with integration order \( d = 0 \).

# ARIMA Model

The **Autoregressive Integrated Moving Average (ARIMA)** model is a flexible and widely used framework for modeling and forecasting univariate time series.\
It captures temporal dependence through three components:\

- an autoregressive (AR) part that models **dependence on past values**, 
- a moving average (MA) part that accounts for **serial correlation in past shocks**, and
- an integration (I) component that **removes non-stationarity through differencing**. 

Once the series is rendered stationary, ARIMA models can be estimated using historical data and selected based on statistical criteria and diagnostic checks to ensure adequate fit and reliable forecasts.

## Splitting Data into Training/Test Sets

To evaluate the out-of-sample forecasting performance of the ARIMA models, the time series is divided into a **training set** and a **test set**.\
The first **80% of the observations** are used for model estimation, while the remaining **20%** are reserved for forecast evaluation.\
This split preserves the temporal ordering of the data, which is essential in time series analysis.

```{r data splitting}
n <- length(ts_data)
train_size <- floor(0.8 * n)

train_data <- ts_data[1:train_size]
test_data  <- ts_data[(train_size + 1):n]

length(train_data)
length(test_data)
```

## `Auto.arima()` model

As an initial benchmark, an ARIMA model is estimated using the automatic model selection procedure provided by `auto.arima()`. This function searches over a range of ARIMA specifications and selects the model that minimizes an information criterion (by default, AICc).

```{r arima_modeling}
model_autoarima <- auto.arima(train_data)
summary(model_autoarima)
```

The automatically selected model is `ARIMA(0,1,1)`, which implies a first difference was applied (`d = 1`) even though the stationarity analysis showed that the series is stationary in levels. In other words, `auto.arima()` unnecessarily differences the series, potentially removing important low-frequency dynamics and increasing noise.

Additionally:\

- The model includes only a single MA(1) term, which may be insufficient to capture the autoregressive structure suggested by the PACF of the original series.

- While the in-sample error metrics (RMSE, MAE) are reasonable, the model may over-difference the series and is less interpretable, leading to suboptimal forecasting performance.

Therefore, although `auto.arima()` provides a convenient starting point, it is not appropriate as the final model. A more rigorous approach, combining a **grid search** over ARMA(p,q) specifications and **residual diagnostics**, is required to select a model that respects the stationarity properties of the series.

## Applying Grid-Search

To select an appropriate ARMA specification for forecasting, a **grid search** was performed over ARMA(p,q) models with \(p \leq 5\) and \(q \leq 5\). Since the series was found to be stationary, no differencing was applied (\(d = 0\)).\
The selection criteria were **AIC** and **BIC**, and the residual diagnostics were used to ensure model adequacy.

```{r grid_search}
# Define maximum AR and MA orders
max_p <- 5
max_q <- 5

# Initialize results dataframe
results <- expand.grid(p = 0:max_p, q = 0:max_q)
results$AIC <- NA
results$BIC <- NA

# Fit ARMA models and store AIC/BIC
for(i in 1:nrow(results)) {
  model <- try(
    Arima(train_data, order = c(results$p[i], 0, results$q[i]), include.mean = TRUE),
    silent = TRUE
  )
  if(!inherits(model, "try-error")) {
    results$AIC[i] <- AIC(model)
    results$BIC[i] <- BIC(model)
  }
}

# Order by AIC
results <- results[order(results$AIC), ]
results

# Identify best models according to AIC and BIC
best_aic <- results[which.min(results$AIC), ]
best_bic <- results[which.min(results$BIC), ]

# Estimate final candidate models
model_aic <- Arima(train_data, order = c(best_aic$p, 0, best_aic$q), include.mean = TRUE)
model_bic <- Arima(train_data, order = c(best_bic$p, 0, best_bic$q), include.mean = TRUE)

summary(model_aic)
summary(model_bic)
```

**Interpretation of Grid Search Results**:\

- The AIC-optimal model is `ARIMA(2,0,0)`, while the BIC-optimal model is `ARIMA(1,0,0)`.
- Both models include a non-zero mean and adequately capture the autoregressive structure of the stationary series.

## Residual Diagnostics

After estimating the candidate ARIMA models, residual diagnostics were performed to ensure that the models adequately capture the dependence structure of the series and that no significant autocorrelation remains.

```{r residuals_diagnostics}
# Residual diagnostics for AIC-optimal model
checkresiduals(model_aic)

# Residual diagnostics for BIC-optimal model
checkresiduals(model_bic)
```

**Interpretation**:\

- Ljungâ€“Box test:
  - `ARIMA(2,0,0)`: Q* = 4.84, df = 8, p-value = 0.775
  - `ARIMA(1,0,0)`: Q* = 6.21, df = 9, p-value = 0.718\
    In both cases, the null hypothesis of **no residual autocorrelation** cannot be rejected, indicating that the **residuals behave like white noise**.

- ACF/PACF of residuals:\
  Both models show no systematic patterns in the autocorrelation or partial autocorrelation functions of the residuals, confirming the absence of remaining serial correlation.

- Model selection implications:\
  The slight difference between AIC and BIC selection is expected: AIC favors slightly more complex models for in-sample fit, while BIC imposes a stronger penalty for model complexity, favoring more parsimonious specifications.


## Final Model Choice

To select an appropriate ARIMA specification for forecasting, a grid search over ARMA(ð‘,q) models with \(p \leq 5\) and \(q \leq 5\) was conducted using the training sample (first 80% of observations). Since the series was found to be stationary, no differencing was applied (\(d=0\)). Model selection was guided by information criteria (AIC, BIC) and residual diagnostics.

The grid search results indicate that the ARIMA(2,0,0) model minimizes the Akaike Information Criterion (AIC), while the ARIMA(1,0,0) model minimizes the Bayesian Information Criterion (BIC). This divergence is expected: AIC favors more flexible models with better in-sample fit, whereas BIC penalizes complexity more heavily, favoring parsimonious models.

Both candidate models were estimated and subjected to residual diagnostics. The Ljungâ€“Box tests fail to reject the null hypothesis of no residual autocorrelation for both ARIMA(2,0,0) and ARIMA(1,0,0) models, indicating that the residuals behave as white noise. Residual ACF and PACF plots also show no systematic patterns, providing additional support for model adequacy.

Initial ACF and PACF plots of the original series suggest a low-order autoregressive dependence structure, with a strong PACF spike at lag 1 and weaker effects at lag 2, consistent with AR(1) or AR(2) candidates. However, visual inspection alone is insufficient to select the optimal model, necessitating formal estimation and information criteria.

Given that both ARIMA(2,0,0) and ARIMA(1,0,0) satisfy all diagnostic checks, the final choice is guided by the principle of parsimony. The **ARIMA(1,0,0)** model (BIC-optimal) is selected as the final forecasting model, balancing simplicity and adequacy while ensuring robust out-of-sample performance.

- **Final model:** `ARIMA(1,0,0)` with non-zero mean  
- **Justification:** Small sample size, parsimony, residuals pass Ljungâ€“Box test, no systematic patterns in ACF/PACF, BIC preference

# Forecast

Using the selected `ARIMA(1,0,0)` model from the previous section, we perform **one-step-ahead recursive forecasts** (expanding-window) for the remaining 20% of the sample. For comparison, we also generate forecasts from the `ARIMA(2,0,0)` model.

```{r one_step_forecast}
n <- length(ts_data)
train_size <- floor(0.8 * n)
test_size <- n - train_size

actuals <- ts_data[(train_size + 1):n]
time_test <- time(ts_data)[(train_size + 1):n]

forecasts_ar1 <- numeric(test_size)
forecasts_ar2 <- numeric(test_size)

for (i in 1:test_size) {
  
  # Expanding window
  train <- ts_data[1:(train_size + i - 1)]
  
  # ARIMA(1,0,0)
  model_ar1 <- Arima(train, order = c(1, 0, 0), include.mean = TRUE)
  forecasts_ar1[i] <- forecast::forecast(model_ar1, h = 1)$mean
  
  # ARIMA(2,0,0)
  model_ar2 <- Arima(train, order = c(2, 0, 0), include.mean = TRUE)
  forecasts_ar2[i] <- forecast::forecast(model_ar2, h = 1)$mean
}

# Plot forecasts
plot(
  time_test, actuals,
  type = "l",
  lwd = 2,
  xlab = "Time",
  ylab = "Net Issuance",
  main = "One-step-ahead Recursive Forecasts"
)

lines(time_test, forecasts_ar1, col = "red", lwd = 2)
lines(time_test, forecasts_ar2, col = "blue", lwd = 2)

legend(
  "topleft",
  legend = c("Actual", "ARIMA(1,0,0)", "ARIMA(2,0,0)"),
  col = c("black", "red", "blue"),
  lwd = 2,
  bty = "n"
)

# Compute RMSE for evaluation
rmse_ar1 <- sqrt(mean((actuals - forecasts_ar1)^2))
rmse_ar2 <- sqrt(mean((actuals - forecasts_ar2)^2))

rmse_ar1
rmse_ar2
```

**Interpretation**:\

- Forecast Behavior
  - Both ARIMA(1,0,0) and ARIMA(2,0,0) forecasts exhibit very similar patterns, consistent with the similarity in in-sample model fit.
  - The forecasts are **relatively smooth and fail to capture abrupt changes** or sharp turning points in the observed data.
  - This is expected given the linear and backward-looking nature of ARIMA models, which underreact to sudden shocks.

- Comparison Between Models
  - ARIMA(2,0,0) reacts slightly faster to changes due to the additional AR term, but the improvement is marginal.
  - Considering parsimony and BIC preference, the **`ARIMA(1,0,0)` model remains the preferable choice** for forecasting.

- Forecast Accuracy
  - RMSE provides a quantitative measure of forecast error. Although the forecasts capture the general trend of the series, large deviations occur during periods of high volatility.
  - The performance is better in stable periods and worsens during abrupt changes, which could be improved by **incorporating external regressors**, **intervention analysis**, or **non-linear/time-varying models**.

- Overall Assessment
  - While both models satisfy the standard residual diagnostics and are therefore statistically adequate, their **out-of-sample forecasting performance is limited in magnitude**. The forecasts capture the general direction of the series but exhibit large deviations from the realized values at several points in the evaluation sample.
  - Despite some limitations in capturing sharp movements, the ARIMA(1,0,0) model provides a statistically adequate and parsimonious forecasting framework.
  - One-step-ahead recursive forecasts are acceptable for **short-term trend estimation** but may require alternative models for precise prediction during volatile periods.


# Further Analysis

This section addresses additional considerations relevant to ARIMA modeling and forecasting.

- **Consequences of Estimating ARIMA on Non-Stationary Data**

  Estimating an ARIMA model on non-stationary data can lead to several serious econometric and forecasting issues:

  - **Unreliable parameter estimates:** Standard inference assumes stationarity. Without it, estimated AR coefficients may appear statistically significant even when no genuine relationship exists, potentially leading to misleading conclusions.  
  - **Invalid inferential statistics:** Non-stationarity violates the assumptions underlying the asymptotic distributions of estimators. T-statistics, confidence intervals, and information criteria (AIC/BIC) may become unreliable, increasing the risk of overfitting. Models may mimic trends rather than capture true dynamics.  
  - **Unstable forecasts:** Forecasts generated from non-stationary data often exhibit explosive behavior or implausible long-term trends. Differencing or detrending is therefore essential to ensure valid inference and stable predictive performance.

  In this analysis, stationarity tests (ADF, PP, KPSS) confirmed that the series is stationary after one differencing step, preventing these issues.

- **Detecting Outliers or Structural Breaks**

  Large outliers or structural breaks can distort parameter estimates and degrade forecast accuracy in ARIMA models. **Standardized residuals** provide a convenient tool to detect such anomalies:

```{r standardized residuals}
res <- residuals(model_ar1)
std_res <- res / sd(res)

plot(std_res, type = "h", main = "Standardized Residuals", ylab = "Standardized Residuals")
abline(h = c(-3, 3), col = "red", lty = 2)
```

  Observations outside Â±3 standard deviations may indicate outliers or temporary shocks.

  In our series, the residual plot shows no extreme deviations, suggesting that the data is free from significant outliers or structural breaks. Therefore, **no additional cleaning was necessary**.

- **Use of Additional Economically Linked Series**

  In practical forecasting, incorporating additional series can enhance predictive accuracy:

  - **Economic relevance**: Variables that are economically linked to the target series (e.g., short-term indicators, related market variables) can capture contemporaneous dynamics.

  - **Timeliness**: If the additional series is published with a shorter lag than the target variable, it provides up-to-date information that can improve forecast responsiveness, especially during periods of rapid change.

  - **Integration in models**: These variables can be included in ARIMAX or regression-augmented ARIMA frameworks to reduce forecast errors and better capture short-term fluctuations.

  Overall, careful consideration of stationarity, data quality, and related economic indicators is essential for robust ARIMA forecasting.